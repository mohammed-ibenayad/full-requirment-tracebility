name: Quality Tracker Test Execution - Per Test Case

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "üéØ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "üìã Test case IDs: $TEST_CASE_IDS"
          echo "üîó GitHub Run ID: $GITHUB_RUN_ID"
          echo "üìù Request ID: $REQUEST_ID"
          echo "üì° Callback URL: $CALLBACK_URL"
          echo "‚ú® NEW: Per test case webhook delivery mode"

      - name: Send initial test status per test case
        if: env.CALLBACK_URL != ''
        run: |
          echo "üì§ Sending initial status for each test case individually..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          
          # Function to send individual test case webhook
          send_test_case_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local logs="$4"
              
              if [ -n "$CALLBACK_URL" ]; then
                  # Clean and escape logs for JSON
                  local escaped_logs=$(echo "$logs" | head -10 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-500)
                  
                  # Create individual test case payload
                  cat > "testcase_${test_id}_payload.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs"
              }
            ]
          }
          EOF
                  
                  echo "üì§ Sending webhook for test case: $test_id (status: $status)"
                  
                  # Send individual webhook
                  HTTP_CODE=$(curl -w "%{http_code}" -o "response_${test_id}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-PerTestCase" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"testcase_${test_id}_payload.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    --retry 2 \
                    --retry-delay 3 \
                    -s)
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "‚úÖ Webhook sent successfully for $test_id: $status (HTTP $HTTP_CODE)"
                  else
                    echo "‚ö†Ô∏è Webhook failed for $test_id with HTTP $HTTP_CODE"
                    cat "response_${test_id}.txt"
                  fi
                  
                  # Small delay between webhook calls to avoid overwhelming server
                  sleep 1
              fi
          }
          
          # Send initial "Not Started" status for each test case
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -n "$test_id" ]; then
                  send_test_case_webhook "$test_id" "Not Started" "0" "Test queued for execution"
              fi
          done
          
          echo "üìä Initial status sent for ${#TEST_ARRAY[@]} test cases individually"

      - name: Run tests with individual webhook updates
        id: run_tests
        run: |
          echo "üöÄ Starting test execution with per-test-case webhook updates..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "üìä Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results
          
          # Function to send individual test case webhook update
          send_test_case_update() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local logs="$4"
              
              if [ -n "$CALLBACK_URL" ]; then
                  # Clean and escape logs for JSON
                  local escaped_logs=$(echo "$logs" | head -20 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)
                  
                  # Create individual test case payload
                  cat > "testcase_${test_id}_update.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs"
              }
            ]
          }
          EOF
                  
                  echo "üì° Updating test case: $test_id -> $status"
                  
                  # Send individual webhook update
                  HTTP_CODE=$(curl -w "%{http_code}" -o "update_response_${test_id}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-PerTestCase" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -H "X-Status-Update: $status" \
                    -d @"testcase_${test_id}_update.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    --retry 2 \
                    --retry-delay 3 \
                    -s)
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "‚úÖ Update sent for $test_id: $status (HTTP $HTTP_CODE)"
                  else
                    echo "‚ö†Ô∏è Update failed for $test_id: $status (HTTP $HTTP_CODE)"
                    cat "update_response_${test_id}.txt"
                  fi
                  
                  # Small delay between updates
                  sleep 0.5
              fi
          }
          
          # Process each test individually with separate webhook calls
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi
              
              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "üß™ [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"
              
              # Send "Running" status update
              send_test_case_update "$test_id" "Running" "0" "Test execution in progress..."
              
              # Run the individual test
              start_time=$(date +%s)
              test_output_file="test-results/output-${test_id}.log"
              
              echo "‚ñ∂Ô∏è  Executing: python -m pytest -v -k \"$test_id\" --tb=short"
              
              if python -m pytest -v -k "$test_id" --tb=short \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="test-results/json-${test_id}.json" \
                  > "$test_output_file" 2>&1; then
                  
                  test_status="Passed"
                  echo "‚úÖ Test PASSED: $test_id"
              else
                  test_status="Failed"
                  echo "‚ùå Test FAILED: $test_id"
              fi
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              
              # Get test output (truncated for JSON)
              if [ -f "$test_output_file" ]; then
                  test_logs=$(cat "$test_output_file")
              else
                  test_logs="No output captured for $test_id"
              fi
              
              # Send final result for this test case
              send_test_case_update "$test_id" "$test_status" "$duration" "$test_logs"
              
              echo "üìä Test completed: $test_id -> $test_status (${duration}s)"
              
              # Small delay to make progress visible and avoid overwhelming webhook server
              sleep 1
          done
          
          echo ""
          echo "üèÅ All individual tests completed with separate webhook updates!"
        continue-on-error: true

      - name: Generate summary for backward compatibility
        run: |
          echo "üìä Generating execution summary..."
          
          # Create a summary file for backward compatibility
          cat > execution_summary.json << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime
          
          # Collect all individual test case results
          results = []
          test_files = glob.glob("testcase_*_update.json")
          
          for test_file in test_files:
              try:
                  with open(test_file, 'r') as f:
                      data = json.load(f)
                      if data.get('results') and len(data['results']) > 0:
                          results.append(data['results'][0])
              except Exception as e:
                  print(f"Error reading {test_file}: {e}")
          
          # Create consolidated summary
          summary = {
              "executionMode": "per-test-case-webhooks",
              "requestId": os.environ.get("REQUEST_ID", ""),
              "requirementId": os.environ.get("REQUIREMENT_ID", ""),
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "totalTests": len(results),
              "results": results,
              "githubRunId": os.environ.get("GITHUB_RUN_ID", ""),
              "note": "Individual test case results were sent via separate webhooks"
          }
          
          with open("execution_summary.json", "w") as f:
              json.dump(summary, f, indent=2)
          
          # Count statuses
          status_counts = {}
          for result in results:
              status = result.get('status', 'Unknown')
              status_counts[status] = status_counts.get(status, 0) + 1
          
          print(f"üìã Execution Summary:")
          print(f"  - Total Tests: {len(results)}")
          for status, count in status_counts.items():
              print(f"  - {status}: {count}")
          EOF
          
          python3 -c "$(cat execution_summary.json)"
          
          echo "‚úÖ Execution summary generated"

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            execution_summary.json
            testcase_*_payload.json
            testcase_*_update.json
            test-results/
          retention-days: 7
          
      - name: Workflow Summary
        run: |
          echo "## üéØ Quality Tracker Per-Test-Case Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Requested Tests:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Mode:** ‚ú® **Per-Test-Case Webhooks** ‚ú®" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f execution_summary.json ]; then
            echo "**Test Results:**" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          try:
              with open('execution_summary.json') as f:
                  data = json.load(f)
              
              results = data.get('results', [])
              status_counts = {}
              for r in results:
                  status = r.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              for status, count in status_counts.items():
                  emoji = '‚úÖ' if status == 'Passed' else '‚ùå' if status == 'Failed' else '‚è≥' if status == 'Not Started' else 'üîÑ' if status == 'Running' else '‚ùì'
                  print(f'- {emoji} **{status}:** {count}')
                  
              print(f'- üìä **Total Webhook Calls:** {len(results) * 2}')  # Initial + Final per test
              print(f'- üîó **Webhook Pattern:** Individual calls per test case')
          except Exception as e:
              print(f'- ‚ùå Error reading summary: {e}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå No execution summary generated" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**New Features:**" >> $GITHUB_STEP_SUMMARY
          echo "- ‚ú® **Individual webhook calls** for each test case" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ **Separate status tracking** per test case" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ **Real-time incremental updates** with composite keys" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ **Progressive status:** Not Started ‚Üí Running ‚Üí Passed/Failed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ **Request isolation** with requestId-testCaseId composite keys" >> $GITHUB_STEP_SUMMARY