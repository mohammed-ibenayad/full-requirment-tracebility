name: Quality Tracker Test Execution - Optimized Per Test Case

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "ðŸŽ¯ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "ðŸ“‹ Test case IDs: $TEST_CASE_IDS"
          echo "ðŸ”— GitHub Run ID: $GITHUB_RUN_ID"
          echo "ðŸ“ Request ID: $REQUEST_ID"
          echo "ðŸ“¡ Callback URL: $CALLBACK_URL"
          echo "âœ¨ OPTIMIZED: Minimal webhook delivery (3 per test case)"

      - name: Initialize consolidated results file
        run: |
          echo "ðŸ“‹ Initializing consolidated results file for artifacts..."
          
          # Create consolidated results JSON for artifact generation
          cat > initialize_results.py << 'EOF'
          import json
          import os
          import time
          
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          
          results = []
          for test_id in test_ids:
              if test_id.strip():
                  results.append({
                      "id": test_id.strip(),
                      "name": f"Test {test_id.strip()}",
                      "status": "Not Started",
                      "duration": 0,
                      "logs": "Test queued for execution"
                  })
          
          # Consolidated format for artifacts (same as original workflow)
          consolidated_payload = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "results": results
          }
          
          # Save consolidated results for artifact generation
          with open("current_results.json", "w") as f:
              json.dump(consolidated_payload, f, indent=2)
          
          print(f"ðŸ“‹ Initialized consolidated results with {len(results)} tests")
          EOF
          
          python initialize_results.py

      - name: Run tests with optimized webhook delivery and enhanced status detection
        id: run_tests
        run: |
          echo "ðŸš€ Starting optimized test execution with minimal webhooks..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "ðŸ“Š Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results
          
          # âœ… OPTIMIZED: Single webhook function - enhanced with additional data while keeping original structure
          # MODIFIED: Added failure_data_json parameter
          send_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local test_logs="$4"
              local failure_data_json="$5" # New parameter for structured failure data
              
              if [ -n "$CALLBACK_URL" ]; then
                  # Clean and escape logs for JSON
                  local escaped_logs=$(echo "$test_logs" | head -20 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)
                  
                  # MODIFIED: Dynamically build the payload to inject failure data using jq
                  local payload_content='{"id": "'"$test_id"'", "name": "Test '"$test_id"'", "status": "'"$status"'", "duration": '$duration', "logs": "'"$escaped_logs"'"}'
                  
                  if [ "$status" = "Failed" ] && [ -n "$failure_data_json" ] && [ "$failure_data_json" != "{}" ]; then
                      # Inject failure, execution, and environment data into the payload
                      payload_content=$(echo "$payload_content" | jq --argjson data "$failure_data_json" '. + $data')
                  fi

                  cat > "webhook_${test_id}_${status}.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              $payload_content
            ]
          }
          EOF
                  
                  echo "ðŸ“¡ Sending webhook: $test_id -> $status"
                  
                  # âœ… SINGLE webhook call with consistent User-Agent (no retries to avoid duplicates)
                  HTTP_CODE=$(curl -w "%{http_code}" -o "response_${test_id}_${status}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"webhook_${test_id}_${status}.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    -s)
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "âœ… Webhook sent: $test_id -> $status (HTTP $HTTP_CODE)"
                  else
                    echo "âš ï¸ Webhook failed: $test_id -> $status (HTTP $HTTP_CODE)"
                    if [ -f "response_${test_id}_${status}.txt" ]; then
                      cat "response_${test_id}_${status}.txt"
                    fi
                  fi
                  
                  # Small delay between webhook calls
                  sleep 0.5
              fi
              
              # âœ… KEEP: Update consolidated results file for artifacts (preserves current behavior)
              cat > update_consolidated_result.py << EOF
          import json
          import os
          import time
          
          # Load current consolidated results
          try:
              with open("current_results.json", "r") as f:
                  payload = json.load(f)
          except:
              payload = {
                  "requirementId": os.environ.get("REQUIREMENT_ID", ""),
                  "requestId": os.environ.get("REQUEST_ID", ""),
                  "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                  "results": []
              }
          
          # Update specific test result in consolidated format
          test_id = "$test_id"
          status = "$status"
          duration = int("$duration" or "0")
          logs = r"""$test_logs"""
          
          # Find and update the test result in consolidated results
          updated = False
          for result in payload["results"]:
              if result["id"] == test_id:
                  result["status"] = status
                  result["duration"] = duration
                  result["logs"] = logs
                  result["name"] = f"Test {test_id}"
                  updated = True
                  break
          
          # If not found, add new result to consolidated results
          if not updated:
              payload["results"].append({
                  "id": test_id,
                  "name": f"Test {test_id}",
                  "status": status,
                  "duration": duration,
                  "logs": logs
              })
          
          # Update timestamp and ensure required fields for artifacts
          payload["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
          payload["requirementId"] = os.environ.get("REQUIREMENT_ID", "")
          payload["requestId"] = os.environ.get("REQUEST_ID", "")
          
          # Save updated consolidated results for artifact generation
          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)
          
          print(f"ðŸ“ Updated consolidated results: {test_id} -> {status}")
          EOF
              
              python update_consolidated_result.py
          }
          
          # MODIFIED: Python script to parse pytest-json-report for detailed failure data
          cat > parse_json_report.py << 'EOF'
import json
import sys
import os

def parse_failure_details(json_report_path, test_id_filter=None):
    try:
        with open(json_report_path, 'r') as f:
            report_data = json.load(f)

        test_case_results = report_data.get('tests', [])
        
        # Find the specific test case result
        test_result = None
        for tr in test_case_results:
            # Check against test_id and potentially nodeid
            if test_id_filter and (tr.get('id') == test_id_filter or test_id_filter in tr.get('nodeid', '')):
                test_result = tr
                break
        if not test_result and test_case_results: # Fallback to first if no filter or not found
            test_result = test_case_results[0]
            
        if not test_result:
            return {}

        failure_data = {}
        # Prioritize call outcome, then setup, then teardown
        outcome_section = None
        if 'call' in test_result and test_result['call'].get('outcome') == 'failed':
            outcome_section = test_result['call']
            failure_data['phase'] = 'call'
        elif 'setup' in test_result and test_result['setup'].get('outcome') == 'failed':
            outcome_section = test_result['setup']
            failure_data['phase'] = 'setup'
        elif 'teardown' in test_result and test_result['teardown'].get('outcome') == 'failed':
            outcome_section = test_result['teardown']
            failure_data['phase'] = 'teardown'

        if outcome_section:
            longrepr = outcome_section.get('longrepr', {})
            failure_data['type'] = longrepr.get('errortype') or 'UnknownError'
            failure_data['rawError'] = longrepr.get('reprcrash', {}).get('message')
            
            # Extract file, line, method, class from nodeid or crash
            nodeid = test_result.get('nodeid', '')
            if nodeid:
                parts = nodeid.split('::')
                if len(parts) >= 1: failure_data['file'] = parts[0]
                if len(parts) >= 2: failure_data['class'] = parts[1]
                if len(parts) >= 3: failure_data['method'] = parts[2]
            
            # If line not found in nodeid, try longrepr
            if 'line' not in failure_data and longrepr.get('reprcrash', {}).get('lineno') is not None:
                failure_data['line'] = longrepr['reprcrash']['lineno']

            # Assertion details
            assertion_data = {"available": False}
            if longrepr.get('sections'):
                for section in longrepr['sections']:
                    if section.get('name') == 'longrepr' and 'content' in section:
                        for line in section['content'].splitlines():
                            if 'assert' in line:
                                assertion_data['available'] = True
                                assertion_data['expression'] = line.strip()
                                # Basic extraction for equality assertions
                                if '==' in line:
                                    try:
                                        parts = line.split('==')
                                        assertion_data['actual'] = parts[0].strip().split('assert ')[-1]
                                        assertion_data['expected'] = parts[1].strip()
                                        assertion_data['operator'] = '=='
                                    except: pass # Ignore if parsing fails
                                break
            failure_data['assertion'] = assertion_data
            
        execution_data = {
            "exitCode": report_data.get('exitcode', 1),
            "framework": "pytest",
            "collected": report_data.get('collected', 0),
            "selected": report_data.get('passed', 0) + report_data.get('failed', 0) + report_data.get('skipped', 0) + report_data.get('xfailed', 0) + report_data.get('xpassed', 0) + report_data.get('errors', 0) , # More accurate selected
            "deselected": report_data.get('deselected', 0),
            "pytestDuration": report_data.get('duration', 0),
            "setupDuration": test_result.get('setup', {}).get('duration', 0),
            "callDuration": test_result.get('call', {}).get('duration', 0),
            "teardownDuration": test_result.get('teardown', {}).get('duration', 0)
        }

        environment_data = report_data.get('_metadata', {})
        # Extract specific environment details
        env_python = environment_data.get('python', {})
        env_pytest = environment_data.get('pytest', {})
        env_platform = environment_data.get('platform', {})

        env_details = {
            "python": env_python.get('version', 'unknown'),
            "pytest": env_pytest.get('version', 'unknown'),
            "platform": env_platform.get('platform', 'unknown'),
            "ci": "GitHub Actions",
            "browser": os.environ.get("BROWSER_USED", "Not Specified") # Add a way to pass browser info if needed
        }

        return {
            "failure": failure_data,
            "execution": execution_data,
            "environment": env_details
        }

    except FileNotFoundError:
        print(f"Error: JSON report not found at {json_report_path}", file=sys.stderr)
        return {}
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in report at {json_report_path}", file=sys.stderr)
        return {}
    except Exception as e:
        print(f"Error parsing JSON report: {e}", file=sys.stderr)
        return {}

if __name__ == "__main__":
    report_path = sys.argv[1]
    test_id_filter = sys.argv[2] if len(sys.argv) > 2 else None
    details = parse_failure_details(report_path, test_id_filter)
    if details:
        print(json.dumps(details))
    else:
        # If no valid details, print an empty JSON object
        print("{}", file=sys.stderr)
EOF
          
          # âœ… OPTIMIZED: Process each test with exactly 3 webhook calls
          for test_id in "${TEST_ARRAY[@]}"; do
            if [ -z "$test_id" ]; then continue; fi
            
            CURRENT_TEST=$((CURRENT_TEST + 1))
            echo ""
            echo "ðŸ§ª [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"
            
            # 1. Send "Not Started" status (ONCE)
            send_webhook "$test_id" "Not Started" "0" "Test queued for execution" "{}" # Empty failure data initially
            
            # 2. Send "Running" status (ONCE)
            send_webhook "$test_id" "Running" "0" "Test execution in progress..." "{}" # Empty failure data initially
            
            # 3. Run the individual test with enhanced status detection
            start_time=$(date +%s)
            test_output_file="test-results/output-${test_id}.log"
            json_report_file="test-results/json-${test_id}.json" # Define json report file path
            
            echo "â–¶ï¸ Â Executing: python -m pytest -v -k \"$test_id\" --tb=short"
            
            # ENHANCED: Capture exit code and detect missing test implementations
            # MODIFIED: Ensure pytest-json-report is always generated
            if python -m pytest -v -k "$test_id" --tb=short \
                --junit-xml="test-results/junit-${test_id}.xml" \
                --json-report --json-report-file="$json_report_file" \
                > "$test_output_file" 2>&1; then
                
                test_status="Passed"
                echo "âœ… Test PASSED: $test_id"
            else
                exit_code=$?
                
                # Check if no tests were collected (exit code 5) or "collected 0 items" in output
                if [ $exit_code -eq 5 ] || grep -q "collected 0 items" "$test_output_file"; then
                    test_status="Not Found"
                    echo "âš ï¸ Â Test implementation NOT FOUND: $test_id"
                    echo "ðŸ’¡ Expected test file or function containing '$test_id' pattern"
                else
                    test_status="Failed"
                    echo "âŒ Test FAILED: $test_id (exit code: $exit_code)"
                fi
            fi
            
            end_time=$(date +%s)
            duration=$((end_time - start_time))
            
            # MODIFIED: Extract comprehensive test data using the new Python script
            test_logs_for_webhook="Test completed successfully." # Default success message
            failure_details_json="{}" # Default empty JSON for failure details

            if [ "$test_status" = "Failed" ]; then
                if [ -f "$json_report_file" ]; then
                    echo "ðŸ“ Extracting detailed failure data from JSON report for $test_id..."
                    # Pass test_id as filter to the Python script
                    extracted_details=$(python parse_json_report.py "$json_report_file" "$test_id")
                    
                    if [ -n "$extracted_details" ] && [ "$extracted_details" != "{}" ]; then
                        failure_details_json="$extracted_details"
                        # Attempt to get a more specific log message from the extracted data
                        extracted_raw_error=$(echo "$extracted_details" | jq -r '.failure.rawError // ""')
                        if [ -n "$extracted_raw_error" ]; then
                            test_logs_for_webhook="FAILED: $extracted_raw_error"
                        fi
                    else
                        echo "âš ï¸ No structured failure details found in JSON report for $test_id, falling back to log file."
                        test_logs_for_webhook="FAILED: $(head -n 20 "$test_output_file" | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)"
                    fi
                else
                    echo "âŒ JSON report not found for $test_id, falling back to log file."
                    test_logs_for_webhook="FAILED: $(head -n 20 "$test_output_file" | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)"
                fi
            elif [ "$test_status" = "Not Found" ]; then
                test_logs_for_webhook="Test implementation not found for pattern: $test_id"
            else # Passed
                test_logs_for_webhook="Test completed successfully."
            fi
            
            # 4. Send final result (ONCE)
            # MODIFIED: Pass the collected failure_details_json as the 5th argument
            send_webhook "$test_id" "$test_status" "$duration" "$test_logs_for_webhook" "$failure_details_json"
            
            echo "ðŸ“Š Test completed: $test_id -> $test_status (${duration}s)"
            
            # Small delay to make progress visible
            sleep 1
          done
          
          echo ""
          echo "ðŸ All tests completed! Expected webhooks: $((TOTAL_TESTS * 3))"
        continue-on-error: true

      - name: Generate final results and cleanup
        run: |
          echo "ðŸ“Š Generating final consolidated results..."
          
          # âœ… KEEP: Generate the essential consolidated results file
          if [ -f "current_results.json" ]; then
            # âœ… FRONTEND COMPATIBLE: Create results with run ID (pattern matching will find it)
            cp current_results.json "results-$GITHUB_RUN_ID.json"
            echo "âœ… Results file created: results-$GITHUB_RUN_ID.json"
            
            # Display final consolidated results
            echo "ðŸ“„ Final consolidated results:"
            cat current_results.json | jq '.' || cat current_results.json
          else
            echo "âŒ No consolidated results file found"
          fi
          
          # âœ… OPTIONAL: Generate execution summary only if needed for analytics
          if [ "${GENERATE_SUMMARY:-false}" = "true" ]; then
            cat > generate_execution_summary.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime
          
          # Collect webhook files for reference
          webhook_files = glob.glob("webhook_*_*.json")
          
          summary = {
              "executionMode": "optimized-minimal-webhooks-with-not-found-detection",
              "requestId": os.environ.get("REQUEST_ID", ""),
              "requirementId": os.environ.get("REQUIREMENT_ID", ""),
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "webhooksSent": len(webhook_files),
              "consolidatedArtifactGenerated": os.path.exists("current_results.json"),
              "githubRunId": os.environ.get("GITHUB_RUN_ID", ""),
              "note": "Enhanced with 'Not Found' status detection for missing test implementations"
          }
          
          # Read consolidated results for summary
          if os.path.exists("current_results.json"):
              with open("current_results.json", "r") as f:
                  consolidated_data = json.load(f)
                  
              results = consolidated_data.get("results", [])
              summary["totalTests"] = len(results)
              
              # Count statuses including new "Not Found" status
              status_counts = {}
              for result in results:
                  status = result.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              summary["statusSummary"] = status_counts
              summary["webhooksPerTest"] = 3
              summary["expectedWebhooks"] = len(results) * 3
              summary["actualWebhooks"] = len(webhook_files)
              summary["efficiency"] = "Optimized" if len(webhook_files) <= len(results) * 3 else "Needs review"
              
              print(f"ðŸ“‹ Execution Summary:")
              print(f" Â - Total Tests: {len(results)}")
              print(f" Â - Expected Webhooks: {len(results) * 3}")
              print(f" Â - Actual Webhooks: {len(webhook_files)}")
              print(f" Â - Efficiency: {summary['efficiency']}")
              for status, count in status_counts.items():
                  print(f" Â - {status}: {count}")
          
          with open("execution_summary.json", "w") as f:
              json.dump(summary, f, indent=2)
          
          print("âœ… Execution summary generated")
          EOF
            
            python generate_execution_summary.py
            echo "ðŸ“Š Execution summary generated (optional analytics)"
          else
            echo "ðŸ“Š Skipping execution summary generation"
          fi
          
          # ðŸ—‘ï¸ CLEANUP: Remove temporary webhook files after successful execution
          echo "ðŸ§¹ Cleaning up temporary files..."
          rm -f webhook_*_*.json response_*_*.txt update_consolidated_result.py generate_execution_summary.py initialize_results.py parse_json_report.py
          echo "âœ… Temporary files cleaned up"

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            results-${{ github.run_id }}.json
            current_results.json
            execution_summary.json
            test-results/
          retention-days: 7
          
      - name: Workflow Summary
        run: |
          echo "## ðŸŽ¯ Quality Tracker Enhanced Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Requested Tests:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Mode:** âœ¨ **Enhanced with 'Not Found' Status Detection** âœ¨" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f current_results.json ]; then
            echo "**Test Results:**" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          try:
              with open('current_results.json') as f:
                  data = json.load(f)
              
              results = data.get('results', [])
              status_counts = {}
              for r in results:
                  status = r.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              for status, count in status_counts.items():
                  if status == 'Passed':
                      emoji = 'âœ…'
                  elif status == 'Failed':
                      emoji = 'âŒ'
                  elif status == 'Not Found':
                      emoji = 'âš ï¸'
                  elif status == 'Not Started':
                      emoji = 'â³'
                  elif status == 'Running':
                      emoji = 'ðŸ”„'
                  else:
                      emoji = 'â“'
                  print(f'- {emoji} **{status}:** {count}')
                  
              # Show enhanced webhook delivery info
              total_tests = len(results)
              expected_webhooks = total_tests * 3
              print(f'- ðŸ“¡ **Webhooks Sent:** {expected_webhooks} (3 per test)')
              print(f'- ðŸ“¦ **Consolidated Artifacts:** Generated')
              print(f'- âš¡ **Enhancement:** Missing test implementation detection')
              print(f'- âœ… **Status Clarity:** Clear distinction between failed vs missing tests')
          except Exception as e:
              print(f'- âŒ Error reading results: {e}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ No consolidated results generated" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Enhancement Features:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Enhanced Status Detection:** Distinguishes between failed tests and missing implementations" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **'Not Found' Status:** Tests without implementations get proper status" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Exit Code Analysis:** Uses pytest exit code 5 to detect missing tests" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Log Pattern Detection:** Checks for 'collected 0 items' as backup detection" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Actionable Feedback:** Clear messaging about what needs to be implemented" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Backward Compatibility:** Same artifact structure as original workflow" >> $GITHUB_STEP_SUMMARY