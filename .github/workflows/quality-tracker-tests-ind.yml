# =============================================================================
# CURRENT WORKFLOW ANALYSIS: quality-tracker-tests-ind.yml
# =============================================================================

# PROBLEM IDENTIFIED: The workflow is sending TOO MANY webhooks per test case
# 
# CURRENT PATTERN (per test case):
# 1. "Not Started" (initial) ‚úÖ NEEDED
# 2. "Running" (before test execution) ‚úÖ NEEDED  
# 3. "Passed/Failed" (after test execution) ‚úÖ NEEDED
# 
# But the workflow is ALSO calling send_dual_update() multiple times which creates:
# - Duplicate webhooks from the same source
# - Unnecessary overhead
# - Backend spam

# =============================================================================
# OPTIMIZED WORKFLOW: MINIMAL WEBHOOKS VERSION
# =============================================================================

name: Quality Tracker Test Execution - Optimized

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "üéØ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "üìã Test case IDs: $TEST_CASE_IDS"
          echo "üîó GitHub Run ID: $GITHUB_RUN_ID"
          echo "üìù Request ID: $REQUEST_ID"
          echo "üì° Callback URL: $CALLBACK_URL"
          echo "‚ú® OPTIMIZED: Minimal webhook delivery (3 per test case)"

      # =========================================================================
      # SIMPLIFIED: Single webhook function with minimal calls
      # =========================================================================
      - name: Run tests with minimal webhooks
        id: run_tests
        run: |
          echo "üöÄ Starting optimized test execution..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "üìä Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results
          
          # ‚úÖ OPTIMIZED: Single webhook function - no duplicates
          send_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local logs="$4"
              
              if [ -n "$CALLBACK_URL" ]; then
                  # Clean and escape logs for JSON
                  local escaped_logs=$(echo "$logs" | head -10 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-500)
                  
                  # Create webhook payload
                  cat > "webhook_${test_id}_${status}.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs"
              }
            ]
          }
          EOF
                  
                  echo "üì° Webhook: $test_id -> $status"
                  
                  # Send webhook (single call, no retries to avoid duplicates)
                  HTTP_CODE=$(curl -w "%{http_code}" -o "response_${test_id}_${status}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-Optimized" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"webhook_${test_id}_${status}.json" \
                    "$CALLBACK_URL" \
                    --max-time 15 \
                    -s)
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "‚úÖ Webhook sent: $test_id -> $status (HTTP $HTTP_CODE)"
                  else
                    echo "‚ö†Ô∏è Webhook failed: $test_id -> $status (HTTP $HTTP_CODE)"
                    # Don't retry to avoid duplicates
                  fi
              fi
          }
          
          # ‚úÖ OPTIMIZED: Process each test with exactly 3 webhooks
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi
              
              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "üß™ [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"
              
              # 1. Send "Not Started" status (ONCE)
              send_webhook "$test_id" "Not Started" "0" "Test queued for execution"
              
              # 2. Send "Running" status (ONCE)  
              send_webhook "$test_id" "Running" "0" "Test execution in progress"
              
              # 3. Run the actual test
              start_time=$(date +%s)
              test_output_file="test-results/output-${test_id}.log"
              
              echo "‚ñ∂Ô∏è  Executing: python -m pytest -v -k \"$test_id\" --tb=short"
              
              if python -m pytest -v -k "$test_id" --tb=short \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="test-results/json-${test_id}.json" \
                  > "$test_output_file" 2>&1; then
                  
                  test_status="Passed"
                  echo "‚úÖ Test PASSED: $test_id"
              else
                  test_status="Failed"
                  echo "‚ùå Test FAILED: $test_id"
              fi
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              
              # Get test output
              if [ -f "$test_output_file" ]; then
                  test_logs=$(cat "$test_output_file")
              else
                  test_logs="No output captured for $test_id"
              fi
              
              # 4. Send final result (ONCE)
              send_webhook "$test_id" "$test_status" "$duration" "$test_logs"
              
              echo "üìä Test completed: $test_id -> $test_status (${duration}s)"
              
              # Small delay between tests
              sleep 1
          done
          
          echo ""
          echo "üèÅ All tests completed! Total webhooks sent: $((TOTAL_TESTS * 3))"
        continue-on-error: true

      # =========================================================================
      # SIMPLIFIED: Generate basic results for artifacts
      # =========================================================================
      - name: Generate results artifacts
        run: |
          echo "üìä Generating results artifacts..."
          
          # Create simple consolidated results
          cat > generate_results.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime
          
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          
          results = []
          
          # Read results from test output files
          for test_id in test_ids:
              if test_id.strip():
                  # Try to determine status from output files
                  status = "Unknown"
                  duration = 0
                  logs = f"Test {test_id.strip()} executed"
                  
                  # Check if junit file exists and passed
                  junit_file = f"test-results/junit-{test_id.strip()}.xml"
                  if os.path.exists(junit_file):
                      with open(junit_file, 'r') as f:
                          junit_content = f.read()
                          if 'failures="0"' in junit_content and 'errors="0"' in junit_content:
                              status = "Passed"
                          else:
                              status = "Failed"
                  
                  results.append({
                      "id": test_id.strip(),
                      "name": f"Test {test_id.strip()}",
                      "status": status,
                      "duration": duration,
                      "logs": logs
                  })
          
          # Create consolidated results
          consolidated_results = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "results": results
          }
          
          # Save results
          with open("results.json", "w") as f:
              json.dump(consolidated_results, f, indent=2)
          
          with open(f"results-{os.environ.get('GITHUB_RUN_ID', 'unknown')}.json", "w") as f:
              json.dump(consolidated_results, f, indent=2)
          
          print(f"üìã Generated results for {len(results)} tests")
          EOF
          
          python generate_results.py

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            results*.json
            test-results/
          retention-days: 7
          
      - name: Workflow Summary
        run: |
          echo "## üéØ Quality Tracker Optimized Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Tests Executed:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Optimization:** ‚úÖ **Minimal Webhooks (3 per test)** ‚úÖ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f results.json ]; then
            echo "**Results:**" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          try:
              with open('results.json') as f:
                  data = json.load(f)
              
              results = data.get('results', [])
              total_tests = len(results)
              total_webhooks = total_tests * 3
              
              status_counts = {}
              for r in results:
                  status = r.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              for status, count in status_counts.items():
                  emoji = '‚úÖ' if status == 'Passed' else '‚ùå' if status == 'Failed' else '‚ùì'
                  print(f'- {emoji} **{status}:** {count}')
              
              print(f'- üì° **Total Webhooks Sent:** {total_webhooks} (3 per test)')
              print(f'- ‚ö° **Efficiency:** Optimized delivery pattern')
          except Exception as e:
              print(f'- ‚ùå Error reading results: {e}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå No results generated" >> $GITHUB_STEP_SUMMARY
          fi

# =============================================================================
# KEY OPTIMIZATIONS MADE
# =============================================================================

# ‚úÖ REMOVED: Dual tracking system (was causing duplicates)
# ‚úÖ REMOVED: Initial status sending loop (redundant) 
# ‚úÖ REMOVED: Consolidated results updates (unnecessary in workflow)
# ‚úÖ SIMPLIFIED: Single webhook function with no retries
# ‚úÖ REDUCED: From 11+ webhooks to exactly 3 per test case
# ‚úÖ MAINTAINED: All essential functionality and artifacts

# WEBHOOK PATTERN PER TEST CASE:
# 1. "Not Started" ‚Üí Test queued
# 2. "Running" ‚Üí Test execution begins  
# 3. "Passed/Failed" ‚Üí Final result
# 
# TOTAL: 3 webhooks per test case (instead of 11+)

# =============================================================================
# WHAT TO UPDATE IN YOUR REPOSITORY
# =============================================================================

# 1. Replace your current quality-tracker-tests-ind.yml with this optimized version
# 2. Update the User-Agent to "GitHub-Actions-Quality-Tracker-Optimized" 
# 3. Test with a single test case first
# 4. Monitor backend logs to confirm only 3 webhooks per test

# EXPECTED BACKEND LOGS AFTER OPTIMIZATION:
# req_123-TC_001: Not Started ‚Üí Running ‚Üí Passed (3 webhooks total)
# Instead of: 11+ webhooks with duplicates and warnings