name: Quality Tracker Test Execution

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "ğŸ¯ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "ğŸ“‹ Test case IDs: $TEST_CASE_IDS"
          echo "ğŸ”— GitHub Run ID: $GITHUB_RUN_ID"
          echo "ğŸ“ Request ID: $REQUEST_ID"
          echo "ğŸ“¡ Callback URL: $CALLBACK_URL"

      - name: Send initial test status
        if: env.CALLBACK_URL != ''
        run: |
          echo "ğŸ“¤ Sending initial test status to Quality Tracker..."
          
          # Parse test case IDs
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          
          # Create initial results JSON with all tests as "Not Started"
          cat > initial_results.py << 'EOF'
          import json
          import os
          import time
          
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          run_id = os.environ.get("GITHUB_RUN_ID", "")
          
          results = []
          for test_id in test_ids:
              if test_id.strip():
                  results.append({
                      "id": test_id.strip(),
                      "name": f"Test {test_id.strip()}",
                      "status": "Not Started",
                      "duration": 0,
                      "logs": "Test queued for execution"
                  })
          
          payload = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "runId": run_id,
              "results": results,
              "metadata": {
                  "totalRequested": len(results),
                  "source": "github-actions-incremental",
                  "phase": "initialization"
              }
          }
          
          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)
          
          print(f"ğŸ“‹ Initialized {len(results)} tests as 'Not Started'")
          EOF
          
          python initial_results.py
          
          # Send initial webhook
          curl -X POST \
            -H "Content-Type: application/json" \
            -H "User-Agent: GitHub-Actions-Quality-Tracker-Incremental" \
            -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
            -H "X-Request-ID: $REQUEST_ID" \
            -d @current_results.json \
            "$CALLBACK_URL" \
            --max-time 30 \
            --retry 2 \
            --retry-delay 5 \
            -s || echo "âš ï¸ Initial status webhook failed"
          
          echo "âœ… Initial status sent"

      - name: Run tests incrementally with live updates
        id: run_tests
        run: |
          echo "ğŸš€ Starting incremental test execution..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "ğŸ“Š Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results
          
          # Function to send webhook update
          send_update() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local logs="$4"
              
              if [ -n "$CALLBACK_URL" ]; then
                  cat > update_result.py << EOF
          import json
          import os
          import time
          
          # Load current results
          try:
              with open("current_results.json", "r") as f:
                  payload = json.load(f)
          except:
              payload = {
                  "requirementId": os.environ.get("REQUIREMENT_ID", ""),
                  "requestId": os.environ.get("REQUEST_ID", ""),
                  "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                  "runId": os.environ.get("GITHUB_RUN_ID", ""),
                  "results": [],
                  "metadata": {"source": "github-actions-incremental"}
              }
          
          # Update specific test result
          test_id = "$test_id"
          status = "$status"
          duration = int("$duration" or "0")
          logs = """$logs"""
          
          # Find and update the test result
          updated = False
          for result in payload["results"]:
              if result["id"] == test_id:
                  result["status"] = status
                  result["duration"] = duration
                  result["logs"] = logs[:500]  # Limit log length
                  result["name"] = f"Test {test_id}"
                  updated = True
                  break
          
          # If not found, add new result
          if not updated:
              payload["results"].append({
                  "id": test_id,
                  "name": f"Test {test_id}",
                  "status": status,
                  "duration": duration,
                  "logs": logs[:500]
              })
          
          # Update timestamp
          payload["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
          payload["metadata"]["phase"] = "execution"
          
          # Save updated results
          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)
          
          print(f"ğŸ“ Updated {test_id}: {status}")
          EOF
                  
                  python update_result.py
                  
                  # Send webhook
                  curl -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-Incremental" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -d @current_results.json \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    --retry 2 \
                    --retry-delay 3 \
                    -s || echo "âš ï¸ Update webhook failed for $test_id"
              fi
          }
          
          # Process each test individually
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi
              
              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "ğŸ§ª [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"
              
              # Update status to "Running"
              send_update "$test_id" "Running" "0" "Test execution in progress..."
              
              # Run the individual test
              start_time=$(date +%s)
              test_output_file="test-results/output-${test_id}.log"
              
              echo "â–¶ï¸  Executing: python -m pytest -v -k \"$test_id\" --tb=short"
              
              if python -m pytest -v -k "$test_id" --tb=short \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="test-results/json-${test_id}.json" \
                  > "$test_output_file" 2>&1; then
                  
                  test_status="Passed"
                  echo "âœ… Test PASSED: $test_id"
              else
                  test_status="Failed"
                  echo "âŒ Test FAILED: $test_id"
              fi
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              
              # Get test output (truncated)
              if [ -f "$test_output_file" ]; then
                  test_logs=$(head -20 "$test_output_file" | tail -10 | tr '\n' ' ' | sed 's/"/\\"/g')
              else
                  test_logs="No output captured"
              fi
              
              # Send final result for this test
              send_update "$test_id" "$test_status" "$duration" "$test_logs"
              
              echo "ğŸ“Š Test completed: $test_id -> $test_status (${duration}s)"
              
              # Small delay to make progress visible
              sleep 1
          done
          
          echo ""
          echo "ğŸ All individual tests completed!"
        continue-on-error: true

      - name: Generate final comprehensive results
        run: |
          echo "ğŸ“Š Generating final comprehensive results..."
          
          cat > generate_final_results.py << 'EOF'
          import json
          import xml.etree.ElementTree as ET
          import os
          import time
          import re
          from pathlib import Path
          import glob
          
          def normalize_test_id(test_id):
              """Normalize test ID for matching (TC-001 -> TC_001)"""
              return re.sub(r'[-_]', '_', test_id.upper()) if test_id else ""
          
          def extract_test_id_from_name(name, requested_ids):
              """Extract test ID from test method/class name"""
              normalized_name = normalize_test_id(name)
              
              for req_id in requested_ids:
                  normalized_req = normalize_test_id(req_id)
                  if normalized_req and normalized_req in normalized_name:
                      return req_id  # Return original format
              return None
          
          # Get environment variables
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          run_id = os.environ.get("GITHUB_RUN_ID", "")
          
          # Parse requested test IDs
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requested_test_ids = [tid.strip() for tid in test_ids if tid.strip()]
          
          print(f"ğŸ” Generating final results for {len(requested_test_ids)} tests")
          
          # Load current incremental results
          final_results = []
          processed_ids = set()
          
          try:
              with open("current_results.json", "r") as f:
                  current_data = json.load(f)
                  final_results = current_data.get("results", [])
                  processed_ids = {r["id"] for r in final_results}
          except:
              print("âš ï¸ No current results found, processing from scratch")
          
          # Process any additional JUnit XML files for missed tests
          junit_files = glob.glob("test-results/junit-*.xml")
          for junit_file in junit_files:
              print(f"ğŸ“„ Processing {junit_file}...")
              try:
                  tree = ET.parse(junit_file)
                  root = tree.getroot()
                  
                  for testcase in root.findall(".//testcase"):
                      name = testcase.get("name", "")
                      time_str = testcase.get("time", "0")
                      
                      # Try to match test ID
                      test_id = extract_test_id_from_name(name, requested_test_ids)
                      if not test_id or test_id in processed_ids:
                          continue
                      
                      # Determine status
                      status = "Passed"
                      logs = f"âœ… Test {test_id} executed successfully"
                      
                      failure = testcase.find("failure")
                      error = testcase.find("error")
                      skipped = testcase.find("skipped")
                      
                      if failure is not None:
                          status = "Failed"
                          logs = f"âŒ FAILURE: {failure.get('message', failure.text or 'Test failed')[:300]}"
                      elif error is not None:
                          status = "Failed"
                          logs = f"ğŸ’¥ ERROR: {error.get('message', error.text or 'Test error')[:300]}"
                      elif skipped is not None:
                          status = "Not Run"
                          logs = f"â­ï¸ SKIPPED: {skipped.get('message', skipped.text or 'Test skipped')[:300]}"
                      
                      # Calculate duration
                      try:
                          duration_ms = int(float(time_str) * 1000)
                      except (ValueError, TypeError):
                          duration_ms = 0
                      
                      final_results.append({
                          "id": test_id,
                          "name": name or f"Test {test_id}",
                          "status": status,
                          "duration": duration_ms,
                          "logs": logs
                      })
                      
                      processed_ids.add(test_id)
                      print(f"âœ… Added missing result: {test_id} -> {status}")
                      
              except Exception as e:
                  print(f"âŒ Error parsing {junit_file}: {e}")
          
          # Add "Not Run" for any completely missing tests
          for test_id in requested_test_ids:
              if test_id not in processed_ids:
                  print(f"ğŸ“ Adding 'Not Run' for missing test: {test_id}")
                  final_results.append({
                      "id": test_id,
                      "name": f"Test {test_id}",
                      "status": "Not Run",
                      "duration": 0,
                      "logs": f"âš ï¸ Test {test_id} was not found or executed"
                  })
          
          # Create final payload
          final_payload = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "runId": run_id,
              "results": final_results,
              "metadata": {
                  "totalRequested": len(requested_test_ids),
                  "totalProcessed": len(final_results),
                  "source": "github-actions-incremental",
                  "phase": "completion",
                  "workflow": "quality-tracker-tests"
              }
          }
          
          # Save final results
          with open("final_results.json", "w") as f:
              json.dump(final_payload, f, indent=2)
          
          # Also save as results.json for backward compatibility
          with open("results.json", "w") as f:
              json.dump(final_payload, f, indent=2)
          
          print(f"ğŸ“Š Final results generated for {len(final_results)} tests")
          
          # Print summary
          status_counts = {}
          for result in final_results:
              status = result["status"]
              status_counts[status] = status_counts.get(status, 0) + 1
          
          print("ğŸ“ˆ Final Test Results Summary:")
          for status, count in status_counts.items():
              emoji = "âœ…" if status == "Passed" else "âŒ" if status == "Failed" else "â­ï¸"
              print(f"  {emoji} {status}: {count}")
          
          EOF
          
          python generate_final_results.py

      - name: Send final completion webhook
        if: env.CALLBACK_URL != ''
        run: |
          echo "ğŸ“¡ Sending final completion status to Quality Tracker..."
          
          if [ -f "final_results.json" ]; then
              echo "ğŸ“„ Final results payload:"
              cat final_results.json | jq '.' || cat final_results.json
              echo ""
              
              # Send final webhook
              HTTP_CODE=$(curl -w "%{http_code}" -o response.txt \
                -X POST \
                -H "Content-Type: application/json" \
                -H "User-Agent: GitHub-Actions-Quality-Tracker-Final" \
                -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                -H "X-Request-ID: $REQUEST_ID" \
                -H "X-Phase: completion" \
                -d @final_results.json \
                "$CALLBACK_URL" \
                --max-time 45 \
                --retry 3 \
                --retry-delay 10 \
                --connect-timeout 15 \
                -s)
              
              echo "ğŸ“Š Final HTTP Response Code: $HTTP_CODE"
              echo "ğŸ“„ Response Body:"
              cat response.txt
              
              if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                echo "âœ… Final webhook sent successfully!"
              else
                echo "âŒ Final webhook failed with HTTP $HTTP_CODE"
              fi
          else
              echo "âŒ No final results file found"
          fi

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            final_results.json
            results.json
            current_results.json
            test-results/
          retention-days: 7
          
      - name: Workflow Summary
        run: |
          echo "## ğŸ¯ Quality Tracker Incremental Test Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Requested Tests:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Mode:** Incremental with real-time updates" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f final_results.json ]; then
            echo "**Final Test Results:**" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('final_results.json') as f:
              data = json.load(f)
          
          results = data.get('results', [])
          status_counts = {}
          for r in results:
              status = r['status']
              status_counts[status] = status_counts.get(status, 0) + 1
          
          for status, count in status_counts.items():
              emoji = 'âœ…' if status == 'Passed' else 'âŒ' if status == 'Failed' else 'â­ï¸'
              print(f'- {emoji} **{status}:** {count}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ No final results file generated" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Features:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Real-time status updates sent to Quality Tracker" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Individual test progress tracking" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Progressive status: Not Started â†’ Running â†’ Passed/Failed" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Comprehensive final results with fallback processing" >> $GITHUB_STEP_SUMMARY