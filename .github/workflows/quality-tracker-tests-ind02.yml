name: Quality Tracker Test Execution - Universal Failure Data

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "ðŸŽ¯ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "ðŸ“‹ Test case IDs: $TEST_CASE_IDS"
          echo "ðŸ”— GitHub Run ID: $GITHUB_RUN_ID"
          echo "ðŸ“ Request ID: $REQUEST_ID"
          echo "ðŸ“¡ Callback URL: ${CALLBACK_URL:-'(not configured)'}"
          echo "ðŸ”„ UNIVERSAL MODE: Failure data extraction for both webhook and polling"

      - name: Initialize consolidated results file
        run: |
          echo "ðŸ“‹ Initializing consolidated results file..."
          
          cat > initialize_results.py << 'EOF'
          import json
          import os
          import time
          
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          
          results = []
          for test_id in test_ids:
              if test_id.strip():
                  results.append({
                      "id": test_id.strip(),
                      "name": f"Test {test_id.strip()}",
                      "status": "Not Started",
                      "duration": 0,
                      "logs": "Test queued for execution"
                  })
          
          consolidated_payload = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "results": results
          }
          
          with open("current_results.json", "w") as f:
              json.dump(consolidated_payload, f, indent=2)
          
          print(f"ðŸ“‹ Initialized consolidated results with {len(results)} tests")
          EOF
          
          python initialize_results.py

      - name: Run tests with universal failure data extraction
        id: run_tests
        run: |
          echo "ðŸš€ Starting test execution with UNIVERSAL failure data extraction..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "ðŸ“Š Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results enhanced-results webhooks-generated
          
          # UNIVERSAL failure data extraction function
          extract_failure_data() {
              local test_id="$1"
              local test_output_file="$2"
              local exit_code="$3"
              
              echo ""
              echo "ðŸ” === FAILURE DATA EXTRACTION FOR $test_id ==="
              
              if [ ! -f "$test_output_file" ]; then
                  echo "âŒ Output file missing: $test_output_file"
                  echo '{"hasFailureData": false, "reason": "no_output_file"}'
                  return
              fi
              
              echo "âœ… Output file exists: $(wc -c < "$test_output_file") bytes"
              
              # Initialize failure data structure
              local error_type=""
              local test_phase="call"
              local error_file=""
              local error_line_num=""
              local test_method=""
              local test_class=""
              local raw_error=""
              local assertion_expr=""
              local assertion_expected=""
              local assertion_actual=""
              local has_failure_data=false
              
              # Extract error type with multiple strategies
              echo "ðŸ” Extracting error type..."
              
              # Strategy 1: E-prefixed exception lines
              error_type=$(grep -E "E\s+.*\.(Exception|Error):" "$test_output_file" | head -1 | sed -E 's/.*\.([A-Za-z]+Exception|[A-Za-z]+Error):.*/\1/')
              [ -n "$error_type" ] && echo "  âœ… Strategy 1 (E-lines): '$error_type'"
              
              # Strategy 2: Selenium exceptions
              if [ -z "$error_type" ]; then
                  error_type=$(grep -E "selenium\.common\.exceptions\.([A-Za-z]+Exception)" "$test_output_file" | head -1 | sed -E 's/.*selenium\.common\.exceptions\.([A-Za-z]+Exception).*/\1/')
                  [ -n "$error_type" ] && echo "  âœ… Strategy 2 (Selenium): '$error_type'"
              fi
              
              # Strategy 3: General exception patterns
              if [ -z "$error_type" ]; then
                  error_type=$(grep -E "([A-Za-z]+Exception|[A-Za-z]+Error):" "$test_output_file" | head -1 | sed -E 's/.*([A-Za-z]+Exception|[A-Za-z]+Error):.*/\1/')
                  [ -n "$error_type" ] && echo "  âœ… Strategy 3 (General): '$error_type'"
              fi
              
              # Strategy 4: Pytest assertion failures
              if [ -z "$error_type" ]; then
                  if grep -q "assert " "$test_output_file"; then
                      error_type="AssertionError"
                      echo "  âœ… Strategy 4 (Assert): '$error_type'"
                  fi
              fi
              
              # Strategy 5: Exit code based
              if [ -z "$error_type" ] && [ "$exit_code" != "0" ]; then
                  error_type="TestFailure"
                  echo "  âœ… Strategy 5 (Exit code): '$error_type'"
              fi
              
              # Extract test phase
              if grep -q "ERROR at setup" "$test_output_file"; then
                  test_phase="setup"
              elif grep -q "ERROR at teardown" "$test_output_file"; then
                  test_phase="teardown"
              fi
              
              # Extract file location with multiple strategies
              error_file=$(grep -E "tests/.*\.py::[A-Za-z_]" "$test_output_file" | head -1 | sed -E 's/(tests\/[^:]+\.py)::.*/\1/')
              if [ -z "$error_file" ]; then
                  error_file=$(grep -E "tests/.*\.py:[0-9]+: in " "$test_output_file" | head -1 | sed -E 's/(tests\/[^:]+\.py):[0-9]+:.*/\1/')
              fi
              if [ -z "$error_file" ]; then
                  error_file=$(grep -E "tests/.*\.py" "$test_output_file" | head -1 | sed -E 's/.*?(tests\/[^:]+\.py).*/\1/')
              fi
              
              # Extract line number
              error_line_num=$(grep -E "tests/.*\.py:[0-9]+: in " "$test_output_file" | head -1 | sed -E 's/tests\/[^:]+\.py:([0-9]+):.*/\1/')
              
              # Extract method and class
              test_method=$(grep -E "tests/.*\.py::[A-Za-z_][^:]*::[A-Za-z_]" "$test_output_file" | head -1 | sed -E 's/.*::([A-Za-z_][A-Za-z0-9_]*) .*/\1/')
              if [ -z "$test_method" ]; then
                  test_method=$(grep -E "tests/.*\.py:[0-9]+: in ([a-zA-Z_][a-zA-Z0-9_]*)" "$test_output_file" | head -1 | sed -E 's/.*: in ([a-zA-Z_][a-zA-Z0-9_]*).*/\1/')
              fi
              
              test_class=$(grep -E "tests/.*\.py::[A-Za-z_][^:]*::" "$test_output_file" | head -1 | sed -E 's/.*::([A-Za-z_][A-Za-z0-9_]*)::.*/\1/')
              
              # Extract raw error with multiple strategies
              raw_error=$(grep -E "^E   " "$test_output_file" | head -3 | sed 's/^E   //' | tr '\n' ' ' | sed 's/"/\\"/g')
              if [ -z "$raw_error" ]; then
                  raw_error=$(grep -A 2 -E "(Exception|Error):" "$test_output_file" | head -2 | tail -1 | sed 's/"/\\"/g')
              fi
              
              # Enhanced assertion extraction
              if grep -q "assert " "$test_output_file"; then
                  assertion_expr=$(grep -A 2 -B 2 "assert " "$test_output_file" | head -1 | sed 's/"/\\"/g')
                  
                  # Look for pytest's detailed assertion output
                  if grep -q "where " "$test_output_file"; then
                      assertion_actual=$(grep "where " "$test_output_file" | head -1 | sed -E 's/.*where .* = (.*)/\1/' | sed 's/"/\\"/g')
                  fi
              fi
              
              # Determine if we have sufficient failure data
              if [ -n "$error_type" ] || [ -n "$raw_error" ] || [ -n "$error_file" ]; then
                  has_failure_data=true
              fi
              
              echo ""
              echo "ðŸ“Š === EXTRACTION SUMMARY ==="
              echo "  - error_type: '${error_type:-N/A}'"
              echo "  - test_phase: '${test_phase:-call}'"
              echo "  - error_file: '${error_file:-N/A}'"
              echo "  - error_line_num: '${error_line_num:-0}'"
              echo "  - test_method: '${test_method:-N/A}'"
              echo "  - test_class: '${test_class:-N/A}'"
              echo "  - raw_error: '${raw_error:0:100}${raw_error:100:1:+...}'"
              echo "  - assertion_expr: '${assertion_expr:-N/A}'"
              echo "  - has_failure_data: $has_failure_data"
              echo "=========================="
              
              # Create failure data JSON
              cat > "enhanced-results/failure_${test_id}.json" << EOF
          {
            "testId": "$test_id",
            "hasFailureData": $has_failure_data,
            "failure": {
              "type": "${error_type:-TestFailure}",
              "phase": "${test_phase:-call}",
              "file": "${error_file:-}",
              "line": ${error_line_num:-0},
              "method": "${test_method:-}",
              "class": "${test_class:-}",
              "rawError": "${raw_error:-}",
              "assertion": {
                "available": $([ -n "$assertion_expr" ] && echo "true" || echo "false"),
                "expression": "${assertion_expr:-}",
                "expected": "${assertion_expected:-}",
                "actual": "${assertion_actual:-}",
                "operator": ""
              }
            },
            "execution": {
              "exitCode": ${exit_code:-1},
              "framework": "pytest",
              "outputFileSize": $(wc -c < "$test_output_file"),
              "outputLines": $(wc -l < "$test_output_file")
            }
          }
          EOF
              
              echo "ðŸ’¾ Failure data saved to: enhanced-results/failure_${test_id}.json"
              echo "ðŸ” === END FAILURE DATA EXTRACTION ==="
              echo ""
          }
          
          # Enhanced webhook function that uses extracted failure data
          send_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local test_logs="$4"
              local test_output_file="$5"
              
              echo "ðŸ“¡ Generating webhook for: $test_id -> $status"
              
              # Clean and escape logs for JSON
              local escaped_logs=$(echo "$test_logs" | head -20 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)
              
              # Check if we have failure data for this test
              local failure_data_file="enhanced-results/failure_${test_id}.json"
              
              if [ "$status" = "Failed" ] && [ -f "$failure_data_file" ]; then
                  echo "âœ… Using extracted failure data from: $failure_data_file"
                  
                  # Read the failure data
                  local failure_json=$(cat "$failure_data_file")
                  local has_failure_data=$(echo "$failure_json" | grep -o '"hasFailureData": *[^,}]*' | sed 's/"hasFailureData": *//')
                  
                  if [ "$has_failure_data" = "true" ]; then
                      echo "âœ… Enhanced failure data available"
                      
                      # Extract failure object from the JSON using Python instead of jq
                      cat > extract_failure_data.py << EOF
import json
import sys

try:
    with open("enhanced-results/failure_${test_id}.json", "r") as f:
        failure_info = json.load(f)
    
    failure_object = failure_info.get("failure", {})
    execution_object = failure_info.get("execution", {})
    
    print("FAILURE_OBJECT_START")
    print(json.dumps(failure_object, indent=2))
    print("FAILURE_OBJECT_END")
    print("EXECUTION_OBJECT_START") 
    print(json.dumps(execution_object, indent=2))
    print("EXECUTION_OBJECT_END")
except Exception as e:
    print(f"ERROR: {e}", file=sys.stderr)
    sys.exit(1)
EOF
                      
                      # Extract the objects
                      python_output=$(python extract_failure_data.py)
                      if [ $? -eq 0 ]; then
                          failure_object=$(echo "$python_output" | sed -n '/FAILURE_OBJECT_START/,/FAILURE_OBJECT_END/p' | sed '1d;$d')
                          execution_object=$(echo "$python_output" | sed -n '/EXECUTION_OBJECT_START/,/EXECUTION_OBJECT_END/p' | sed '1d;$d')
                      
                          # Create enhanced webhook with failure data
                          cat > "webhooks-generated/webhook_${test_id}_${status}.json" << EOF
{
  "requestId": "$REQUEST_ID",
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "results": [
    {
      "id": "$test_id",
      "name": "Test $test_id",
      "status": "$status",
      "duration": $duration,
      "logs": "$escaped_logs",
      "failure": $failure_object,
      "execution": $execution_object
    }
  ]
}
EOF
                          echo "âœ… Enhanced webhook payload created with failure details"
                      else
                          echo "âŒ Failed to extract failure data, using basic payload"
                          
                          cat > "webhooks-generated/webhook_${test_id}_${status}.json" << EOF
{
  "requestId": "$REQUEST_ID",
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "results": [
    {
      "id": "$test_id",
      "name": "Test $test_id",
      "status": "$status",
      "duration": $duration,
      "logs": "$escaped_logs"
    }
  ]
}
EOF
                      fi
                  else
                      echo "âš ï¸  No failure data extracted, using basic payload"
                      
                      cat > "webhooks-generated/webhook_${test_id}_${status}.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs"
              }
            ]
          }
          EOF
                  fi
              else
                  echo "â„¹ï¸  Basic webhook for non-failed test or missing failure data"
                  
                  cat > "webhooks-generated/webhook_${test_id}_${status}.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs"
              }
            ]
          }
          EOF
              fi
              
              # Send webhook if URL is configured
              if [ -n "$CALLBACK_URL" ]; then
                  echo "ðŸ“¡ Sending webhook to: $CALLBACK_URL"
                  
                  HTTP_CODE=$(curl -w "%{http_code}" -o "webhooks-generated/response_${test_id}_${status}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-Universal" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"webhooks-generated/webhook_${test_id}_${status}.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    -s)
                  
                  echo "ðŸ“¡ Webhook HTTP response: $HTTP_CODE"
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                      echo "âœ… Webhook sent successfully"
                  else
                      echo "âŒ Webhook failed (HTTP $HTTP_CODE)"
                  fi
              else
                  echo "ðŸ“¡ No CALLBACK_URL - webhook saved locally only"
              fi
              
              # Update consolidated results (IMPORTANT for polling mechanism)
              cat > update_consolidated_result.py << EOF
          import json
          import os
          import time
          
          try:
              with open("current_results.json", "r") as f:
                  payload = json.load(f)
          except:
              payload = {
                  "requirementId": os.environ.get("REQUIREMENT_ID", ""),
                  "requestId": os.environ.get("REQUEST_ID", ""),
                  "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                  "results": []
              }
          
          test_id = "$test_id"
          status = "$status"
          duration = int("$duration" or "0")
          logs = r"""$test_logs"""
          
          # Load failure data if available
          failure_data = None
          execution_data = None
          try:
              with open("enhanced-results/failure_${test_id}.json", "r") as f:
                  failure_info = json.load(f)
                  if failure_info.get("hasFailureData"):
                      failure_data = failure_info.get("failure")
                      execution_data = failure_info.get("execution")
          except:
              pass
          
          # Find and update the test result
          updated = False
          for result in payload["results"]:
              if result["id"] == test_id:
                  result["status"] = status
                  result["duration"] = duration
                  result["logs"] = logs
                  result["name"] = f"Test {test_id}"
                  
                  # Add failure data if available (CRITICAL for polling)
                  if failure_data:
                      result["failure"] = failure_data
                  if execution_data:
                      result["execution"] = execution_data
                      
                  updated = True
                  break
          
          if not updated:
              new_result = {
                  "id": test_id,
                  "name": f"Test {test_id}",
                  "status": status,
                  "duration": duration,
                  "logs": logs
              }
              
              # Add failure data to new result if available
              if failure_data:
                  new_result["failure"] = failure_data
              if execution_data:
                  new_result["execution"] = execution_data
                  
              payload["results"].append(new_result)
          
          payload["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
          payload["requirementId"] = os.environ.get("REQUIREMENT_ID", "")
          payload["requestId"] = os.environ.get("REQUEST_ID", "")
          
          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)
          
          print(f"ðŸ“ Updated consolidated results: {test_id} -> {status}")
          if failure_data:
              print(f"   âœ… Included failure data for polling mechanism")
          EOF
              
              python update_consolidated_result.py
          }
          
          # Process each test
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi
              
              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "ðŸ§ª [$CURRENT_TEST/$TOTAL_TESTS] ===== PROCESSING TEST: $test_id ====="
              
              # Define output file path
              test_output_file="test-results/output-${test_id}.log"
              
              # 1. Send "Not Started" status
              send_webhook "$test_id" "Not Started" "0" "Test queued for execution" ""
              
              # 2. Send "Running" status  
              send_webhook "$test_id" "Running" "0" "Test execution in progress..." ""
              
              # 3. Run the test and capture detailed output
              start_time=$(date +%s)
              
              echo "â–¶ï¸  Executing pytest for test: $test_id"
              
              if python -m pytest -v -k "$test_id" --tb=long --showlocals \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="test-results/json-${test_id}.json" \
                  > "$test_output_file" 2>&1; then
                  
                  test_status="Passed"
                  test_exit_code=0
                  echo "âœ… Test PASSED: $test_id"
              else
                  test_exit_code=$?
                  
                  if [ $test_exit_code -eq 5 ] || grep -q "collected 0 items" "$test_output_file"; then
                      test_status="Not Found"
                      echo "âš ï¸  Test implementation NOT FOUND: $test_id"
                  else
                      test_status="Failed"
                      echo "âŒ Test FAILED: $test_id (exit code: $test_exit_code)"
                      
                      # CRITICAL: Extract failure data for failed tests
                      extract_failure_data "$test_id" "$test_output_file" "$test_exit_code"
                  fi
              fi
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              
              # Build test logs
              if [ "$test_status" = "Failed" ]; then
                  if [ -f "$test_output_file" ]; then
                      error_summary=$(grep -E "^E   " "$test_output_file" | head -1 | sed 's/^E   //')
                      test_logs="FAILED: ${error_summary:-Test execution failed}"
                  else
                      test_logs="FAILED: Test execution failed (no output file)"
                  fi
              elif [ "$test_status" = "Not Found" ]; then
                  test_logs="Test implementation not found for pattern: $test_id"
              else
                  test_logs="Test completed successfully"
              fi
              
              # 4. Send final result with failure data (if extracted)
              send_webhook "$test_id" "$test_status" "$duration" "$test_logs" "$test_output_file"
              
              echo "ðŸ“Š Test completed: $test_id -> $test_status (${duration}s)"
              echo "ðŸ§ª ===== TEST PROCESSING COMPLETE ====="
              sleep 1
          done
          
          echo ""
          echo "ðŸ All tests completed!"
        continue-on-error: true

      - name: Create enhanced results summary for polling compatibility
        run: |
          echo "ðŸ“Š Creating enhanced results summary compatible with both webhook and polling..."
          
          # Create enhanced artifact files for polling mechanism
          cat > create_enhanced_artifacts.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime
          
          # Load current consolidated results
          with open("current_results.json", "r") as f:
              consolidated_data = json.load(f)
          
          results = consolidated_data.get("results", [])
          
          # Count different result types
          stats = {
              "total": len(results),
              "passed": 0,
              "failed": 0,
              "notFound": 0,
              "failedWithData": 0,
              "other": 0
          }
          
          enhanced_results = []
          
          print("ðŸ” Processing results for enhanced artifact...")
          
          for result in results:
              status = result.get("status", "Unknown")
              test_id = result.get("id", "unknown")
              
              # Count by status
              if status == "Passed":
                  stats["passed"] += 1
              elif status == "Failed":
                  stats["failed"] += 1
                  if "failure" in result:
                      stats["failedWithData"] += 1
              elif status == "Not Found":
                  stats["notFound"] += 1
              else:
                  stats["other"] += 1
              
              # Ensure all results have proper structure for both webhook and polling
              enhanced_result = {
                  "id": result.get("id"),
                  "name": result.get("name", f"Test {result.get('id')}"),
                  "status": result.get("status"),
                  "duration": result.get("duration", 0),
                  "logs": result.get("logs", "")
              }
              
              # Include failure data if present (CRITICAL for polling)
              if "failure" in result:
                  enhanced_result["failure"] = result["failure"]
                  print(f"âœ… {test_id}: Including failure data in enhanced artifact")
              else:
                  print(f"â„¹ï¸  {test_id}: No failure data to include")
              
              if "execution" in result:
                  enhanced_result["execution"] = result["execution"]
              
              enhanced_results.append(enhanced_result)
          
          # Create the enhanced artifact structure
          enhanced_artifact = {
              "requirementId": consolidated_data.get("requirementId"),
              "requestId": consolidated_data.get("requestId"),
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "results": enhanced_results,
              "summary": {
                  "executionMode": "universal-failure-data-extraction",
                  "deliveryMethods": ["webhook", "polling"],
                  "stats": stats,
                  "failureDataSuccessRate": round((stats["failedWithData"] / stats["failed"]) * 100, 1) if stats["failed"] > 0 else 0
              }
          }
          
          # Save enhanced artifact for polling mechanism
          with open("enhanced_results.json", "w") as f:
              json.dump(enhanced_artifact, f, indent=2)
          
          # Also create the standard results file for backward compatibility
          github_run_id = os.environ.get("GITHUB_RUN_ID", "")
          if github_run_id:
              with open(f"results-{github_run_id}.json", "w") as f:
                  json.dump(enhanced_artifact, f, indent=2)
              print(f"âœ… Created standard results file: results-{github_run_id}.json")
          
          print("ðŸ“Š Enhanced Artifact Summary:")
          print(f"  - Total Tests: {stats['total']}")
          print(f"  - Passed: {stats['passed']}")
          print(f"  - Failed: {stats['failed']}")
          print(f"  - Failed with Failure Data: {stats['failedWithData']}")
          print(f"  - Not Found: {stats['notFound']}")
          print(f"  - Other: {stats['other']}")
          
          if stats['failed'] > 0:
              success_rate = (stats['failedWithData'] / stats['failed']) * 100
              print(f"  - Failure Data Success Rate: {success_rate:.1f}%")
              
              if success_rate == 100:
                  print("ðŸŽ‰ SUCCESS: All failed tests have failure data for both webhook and polling!")
              elif success_rate > 0:
                  print(f"âš ï¸  PARTIAL: {stats['failedWithData']}/{stats['failed']} failed tests have failure data")
              else:
                  print("âŒ ISSUE: No failed tests have failure data")
          
          print("âœ… Enhanced results artifact created")
          EOF
          
          python create_enhanced_artifacts.py

      - name: Upload comprehensive artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-universal-${{ github.run_id }}
          path: |
            results-${{ github.run_id }}.json
            current_results.json
            enhanced_results.json
            test-results/
            enhanced-results/
            webhooks-generated/
          retention-days: 7
          
      - name: Universal Workflow Summary
        run: |
          echo "## ðŸ”„ Quality Tracker Universal Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Requested Tests:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Callback URL:** ${CALLBACK_URL:-'(not configured - polling only)'}" >> $GITHUB_STEP_SUMMARY
          echo "**Delivery Methods:** Webhook + Polling Compatible" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f enhanced_results.json ]; then
            echo "**Universal Results Summary:**" >> $GITHUB_STEP_SUMMARY
            cat > analyze_results.py << 'EOF'
import json
import os

try:
    with open('enhanced_results.json') as f:
        data = json.load(f)
    
    summary = data.get('summary', {})
    stats = summary.get('stats', {})
    
    print(f"- ðŸ“Š **Total Tests:** {stats.get('total', 0)}")
    print(f"- âœ… **Passed:** {stats.get('passed', 0)}")
    print(f"- âŒ **Failed:** {stats.get('failed', 0)}")
    print(f"- ðŸ” **Failed with Failure Data:** {stats.get('failedWithData', 0)}")
    print(f"- âš ï¸ **Not Found:** {stats.get('notFound', 0)}")
    print(f"- ðŸ“ˆ **Failure Data Success Rate:** {summary.get('failureDataSuccessRate', 0)}%")
    
    # Show delivery method status
    failed = stats.get('failed', 0)
    with_data = stats.get('failedWithData', 0)
    
    if failed > 0:
        if with_data == failed:
            print("- ðŸŽ‰ **Status:** SUCCESS - All failed tests have failure data")
        elif with_data > 0:
            print(f"- âš ï¸ **Status:** PARTIAL - {with_data}/{failed} failed tests have failure data")
        else:
            print("- âŒ **Status:** ISSUE - No failed tests have failure data")
    
    print("")
    print("**Delivery Method Compatibility:**")
    callback_url = os.environ.get('CALLBACK_URL', '')
    if callback_url:
        print("- ðŸ“¡ **Webhook Delivery:** Enabled")
    else:
        print("- ðŸ“¡ **Webhook Delivery:** Disabled (no URL configured)")
    print("- ðŸ”„ **Polling Compatibility:** Enabled (failure data in artifacts)")
    print("- ðŸ“¦ **Artifact Structure:** Enhanced with failure details")
    
    print("")
    print("**Individual Test Results:**")
    for result in data.get('results', []):
        test_id = result.get('id', 'unknown')
        status = result.get('status', 'unknown')
        has_failure = 'failure' in result
        
        if status == 'Failed':
            if has_failure:
                failure_type = result.get('failure', {}).get('type', 'N/A')
                print(f"- âŒ **{test_id}:** Failed with failure data (Type: {failure_type})")
            else:
                print(f"- âŒ **{test_id}:** Failed but no failure data extracted")
        elif status == 'Passed':
            print(f"- âœ… **{test_id}:** Passed")
        elif status == 'Not Found':
            print(f"- âš ï¸ **{test_id}:** Not Found")
        else:
            print(f"- â„¹ï¸ **{test_id}:** {status}")
            
except Exception as e:
    print(f"- âŒ Error reading enhanced results: {e}")
EOF
            
            python3 analyze_results.py >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ No enhanced results generated" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Universal Features:**" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”„ **Dual Delivery Support:** Works with both webhook and polling mechanisms" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ” **Advanced Failure Extraction:** 5 different strategies for error detection" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“Š **Comprehensive Data:** Extracts error type, file location, method, raw error" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“¦ **Enhanced Artifacts:** Failure data included in downloadable artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸŽ¯ **Backward Compatible:** Works with existing polling and webhook systems" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“¡ **Smart Webhook Generation:** Only sends enhanced payloads when failure data exists" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”¬ **Detailed Analysis:** Individual failure data files for debugging" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“ˆ **Success Metrics:** Tracks failure data extraction success rate" >> $GITHUB_STEP_SUMMARY