name: Quality Tracker Test Execution - Simplified (Phase 1)

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Display test execution info
        run: |
          echo "ðŸŽ¯ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "ðŸ“‹ Test case IDs: $TEST_CASE_IDS"
          echo "ðŸ”— GitHub Run ID: $GITHUB_RUN_ID"
          echo "ðŸ“ Request ID: $REQUEST_ID"
          echo "ðŸ“¡ Callback URL: $CALLBACK_URL"
          echo "âœ¨ PHASE 1: Raw data capture only - frontend handles all parsing"

      - name: Initialize results tracking
        run: |
          echo "ðŸ“‹ Initializing results tracking..."
          
          cat > initialize_results.py << 'EOF'
          import json
          import os
          import time
          
          test_ids = os.environ.get("TEST_CASE_IDS", "").split()
          requirement_id = os.environ.get("REQUIREMENT_ID", "")
          request_id = os.environ.get("REQUEST_ID", "")
          
          results = []
          for test_id in test_ids:
              if test_id.strip():
                  results.append({
                      "id": test_id.strip(),
                      "name": f"Test {test_id.strip()}",
                      "status": "Not Started",
                      "duration": 0,
                      "logs": "",
                      "rawOutput": ""
                  })
          
          consolidated_payload = {
              "requirementId": requirement_id,
              "requestId": request_id,
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "results": results
          }
          
          with open("current_results.json", "w") as f:
              json.dump(consolidated_payload, f, indent=2)
          
          print(f"ðŸ“‹ Initialized tracking for {len(results)} tests")
          EOF
          
          python initialize_results.py

      - name: Run tests with simplified webhook delivery
        id: run_tests
        run: |
          echo "ðŸš€ Starting simplified test execution..."
          
          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0
          
          echo "ðŸ“Š Total tests to run: $TOTAL_TESTS"
          
          # Create test results directory
          mkdir -p test-results
          
          # Fix 3: Simplified send_webhook Function
          send_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local raw_output="$4"
              
              echo "ðŸ“¡ Sending webhook: $test_id -> $status"
              
              if [ -n "$CALLBACK_URL" ]; then
                  # Create webhook payload with proper JSON escaping
                  python3 -c "import json; import sys; import os; data = { 'requestId': os.environ.get('REQUEST_ID', ''), 'timestamp': '$(date -u +"%Y-%m-%dT%H:%M:%SZ")', 'results': [{'id': '$test_id', 'name': 'Test $test_id', 'status': '$status', 'duration': $duration, 'logs': 'Basic execution info', 'rawOutput': '''$raw_output'''}]}; with open('webhook_${test_id}_${status}.json', 'w') as f: json.dump(data, f, indent=2);"
                  
                  # Send webhook
                  HTTP_CODE=$(curl -w "%{http_code}" -o "response_${test_id}_${status}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker-Simplified" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"webhook_${test_id}_${status}.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    -s)
                  
                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "âœ… Webhook sent successfully (HTTP $HTTP_CODE)"
                  else
                    echo "âŒ Webhook failed (HTTP $HTTP_CODE)"
                  fi
                  
                  sleep 0.5
              else
                  echo "âŒ No CALLBACK_URL configured"
              fi
              
              # Update consolidated results
              python3 -c "import json; import os; import time; try: with open('current_results.json', 'r') as f: payload = json.load(f); except: payload = { 'requirementId': os.environ.get('REQUIREMENT_ID', ''), 'requestId': os.environ.get('REQUEST_ID', ''), 'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'results': [] }; test_id = '$test_id'; status = '$status'; duration = $duration; raw_output = '''$raw_output'''; updated = False; for result in payload['results']: if result['id'] == test_id: result['status'] = status; result['duration'] = duration; result['logs'] = f'Test {status.lower()}'; result['rawOutput'] = raw_output; updated = True; break; if not updated: payload['results'].append({'id': test_id, 'name': f'Test {test_id}', 'status': status, 'duration': duration, 'logs': f'Test {status.lower()}', 'rawOutput': raw_output}); payload['timestamp'] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()); with open('current_results.json', 'w') as f: json.dump(payload, f, indent=2); print(f'Updated results: {test_id} -> {status}');"
          }
          
          # SIMPLIFIED: Process each test with 3 webhook calls (no parsing)
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi
              
              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "ðŸ§ª [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"
              
              # Define output file path
              test_output_file="test-results/output-${test_id}.log"
              
              # 1. Send "Not Started" status
              send_webhook "$test_id" "Not Started" "0" ""
              
              # 2. Send "Running" status  
              send_webhook "$test_id" "Running" "0" ""
              
              # Fix 4: Proper Raw Output Capture
              start_time=$(date +%s)
              echo "â–¶ï¸ Executing: python -m pytest -v -k \"$test_id\" --tb=long"
              
              if python -m pytest -v -k "$test_id" --tb=long \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="test-results/json-${test_id}.json" \
                  > "$test_output_file" 2>&1; then
                  
                  test_status="Passed"
                  echo "âœ… Test PASSED: $test_id"
              else
                  exit_code=$?
                  
                  if [ $exit_code -eq 5 ] || grep -q "collected 0 items" "$test_output_file"; then
                      test_status="Not Found"
                      echo "âš ï¸ Test implementation NOT FOUND: $test_id"
                  else
                      test_status="Failed"
                      echo "âŒ Test FAILED: $test_id (exit code: $exit_code)"
                  fi
              fi
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              
              # Read raw output safely
              raw_output=""
              if [ -f "$test_output_file" ]; then
                  raw_output=$(cat "$test_output_file")
              fi
              
              # Send final result with raw output
              send_webhook "$test_id" "$test_status" "$duration" "$raw_output"
              
              echo "ðŸ“Š Test completed: $test_id -> $test_status (${duration}s)"
              sleep 1
          done
          
          echo ""
          echo "ðŸ All tests completed! Total webhooks sent: $((TOTAL_TESTS * 3))"
        continue-on-error: true

      - name: Generate final results
        run: |
          echo "ðŸ“Š Generating final consolidated results..."
          
          if [ -f "current_results.json" ]; then
            cp current_results.json "results-$GITHUB_RUN_ID.json"
            echo "âœ… Results file created: results-$GITHUB_RUN_ID.json"
            
            echo "ðŸ“„ Final consolidated results:"
            cat current_results.json | jq '.' || cat current_results.json
          else
            echo "âŒ No consolidated results file found"
          fi
          
          # Generate simple execution summary
          cat > generate_summary.py << 'EOF'
          import json
          import os
          from datetime import datetime
          
          summary = {
              "executionMode": "simplified-raw-data-only",
              "requestId": os.environ.get("REQUEST_ID", ""),
              "requirementId": os.environ.get("REQUIREMENT_ID", ""),
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "githubRunId": os.environ.get("GITHUB_RUN_ID", ""),
              "note": "Phase 1: Raw data only - all parsing handled by frontend"
          }
          
          if os.path.exists("current_results.json"):
              with open("current_results.json", "r") as f:
                  consolidated_data = json.load(f)
                  
              results = consolidated_data.get("results", [])
              summary["totalTests"] = len(results)
              
              status_counts = {}
              for result in results:
                  status = result.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              summary["statusSummary"] = status_counts
              summary["webhooksPerTest"] = 3
              summary["totalWebhooks"] = len(results) * 3
              
              print(f"ðŸ“‹ Execution Summary:")
              print(f"  - Total Tests: {len(results)}")
              print(f"  - Total Webhooks: {len(results) * 3}")
              for status, count in status_counts.items():
                  print(f"  - {status}: {count}")
          
          with open("execution_summary.json", "w") as f:
              json.dump(summary, f, indent=2)
          
          print("âœ… Summary generated")
          EOF
          
          python generate_summary.py
          
          # Cleanup temporary files
          echo "ðŸ§¹ Cleaning up temporary files..."
          rm -f webhook_*_*.json response_*_*.txt update_consolidated_result.py generate_summary.py initialize_results.py
          echo "âœ… Cleanup completed"

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            results-${{ github.run_id }}.json
            current_results.json
            execution_summary.json
            test-results/
          retention-days: 7
          
      - name: Workflow Summary
        run: |
          echo "## ðŸŽ¯ Quality Tracker Simplified Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirement:** $REQUIREMENT_ID - $REQUIREMENT_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Request ID:** $REQUEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Requested Tests:** $TEST_CASE_IDS" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Mode:** âœ¨ **Phase 1: Simplified - Raw Data Only** âœ¨" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f current_results.json ]; then
            echo "**Test Results:**" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          try:
              with open('current_results.json') as f:
                  data = json.load(f)
              
              results = data.get('results', [])
              status_counts = {}
              for r in results:
                  status = r.get('status', 'Unknown')
                  status_counts[status] = status_counts.get(status, 0) + 1
              
              for status, count in status_counts.items():
                  if status == 'Passed':
                      emoji = 'âœ…'
                  elif status == 'Failed':
                      emoji = 'âŒ'
                  elif status == 'Not Found':
                      emoji = 'âš ï¸'
                  elif status == 'Not Started':
                      emoji = 'â³'
                  elif status == 'Running':
                      emoji = 'ðŸ”„'
                  else:
                      emoji = 'â“'
                  print(f'- {emoji} **{status}:** {count}')
                  
              total_tests = len(results)
              print(f'- ðŸ“¡ **Webhooks Sent:** {total_tests * 3} (3 per test)')
              print(f'- ðŸ“¦ **Raw Data Captured:** All test output preserved for frontend parsing')
              print(f'- âš¡ **Phase 1 Benefits:** Simplified workflow, deterministic frontend parsing')
          except Exception as e:
              print(f'- âŒ Error reading results: {e}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ No consolidated results generated" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Phase 1 Improvements:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Simplified Workflow:** Focus only on test execution and raw data capture" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Framework Agnostic:** No assumptions about testing frameworks or languages" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Deterministic Parsing:** Frontend uses reliable patterns for error extraction" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Raw Output Preservation:** Complete test output available for frontend analysis" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Faster Execution:** Reduced workflow processing time and complexity" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Better Separation:** Clear distinction between execution and analysis concerns" >> $GITHUB_STEP_SUMMARY
