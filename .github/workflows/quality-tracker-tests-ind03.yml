name: Quality Tracker Test Execution - Enhanced with Artifact Failure Objects

on:
  repository_dispatch:
    types: [quality-tracker-test-run]

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      REQUIREMENT_ID: ${{ github.event.client_payload.requirementId }}
      REQUIREMENT_NAME: ${{ github.event.client_payload.requirementName }}
      TEST_CASE_IDS: ${{ join(github.event.client_payload.testCases, ' ') }}
      CALLBACK_URL: ${{ github.event.client_payload.callbackUrl }}
      GITHUB_RUN_ID: ${{ github.run_id }}
      REQUEST_ID: ${{ github.event.client_payload.requestId }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html pytest-json-report
          pip install selenium webdriver-manager
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Display test execution info
        run: |
          echo "ğŸ¯ Executing tests for requirement: $REQUIREMENT_ID - $REQUIREMENT_NAME"
          echo "ğŸ“‹ Test case IDs: $TEST_CASE_IDS"
          echo "ğŸŒ Callback URL: $CALLBACK_URL"
          echo "ğŸ†” GitHub Run ID: $GITHUB_RUN_ID"
          echo "ğŸ“ Request ID: $REQUEST_ID"

      - name: Create initial consolidated results file
        id: initialize_results
        run: |
          python - <<EOF
          import json
          import os
          import time

          payload = {
              "requirementId": os.environ.get("REQUIREMENT_ID", ""),
              "requestId": os.environ.get("REQUEST_ID", ""),
              "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
              "metadata": {
                  "workflowRunId": os.environ.get("GITHUB_RUN_ID", ""),
                  "executionMode": "enhanced-with-failure-objects",
                  "totalTests": 0,
                  "enhancedArtifacts": True
              },
              "results": []
          }

          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)

          print("âœ… Initial consolidated results file created: current_results.json")
          EOF

      - name: Run tests with enhanced artifact generation
        id: run_tests
        run: |
          echo "ğŸš€ Starting enhanced test execution with failure object preservation..."

          # Parse test case IDs into array
          IFS=' ' read -ra TEST_ARRAY <<< "$TEST_CASE_IDS"
          TOTAL_TESTS=${#TEST_ARRAY[@]}
          CURRENT_TEST=0

          echo "ğŸ“Š Total tests to run: $TOTAL_TESTS"

          # Create test results directory
          mkdir -p test-results

          # ENHANCED: Extract failure object for both webhooks AND artifacts using Python
          # This function will now read from the pytest JSON report
          cat > extract_failure_object.py << 'EOF'
          import json
          import os
          import sys

          test_id = os.environ.get("CURRENT_TEST_ID")
          json_report_file = os.environ.get("CURRENT_JSON_REPORT_FILE")
          test_output_file = os.environ.get("CURRENT_TEST_OUTPUT_FILE") # Still useful for raw logs
          status = os.environ.get("CURRENT_TEST_STATUS")

          failure_object = None

          if status == "Failed" and os.path.exists(json_report_file):
              try:
                  with open(json_report_file, 'r') as f:
                      report_data = json.load(f)

                  # Find the specific test case's failure details
                  for test_result in report_data.get('tests', []):
                      # Use 'nodeid' which is like 'tests/test_user.py::TestOpenCart::test_homepage_loads_TC_001'
                      # We need to match the test_id from the end of the nodeid
                      if test_result.get('nodeid') and test_result['nodeid'].endswith(f"::{test_id}"):
                          # Check for setup/call/teardown errors
                          for phase in ['setup', 'call', 'teardown']:
                              if test_result.get(phase) and test_result[phase].get('outcome') == 'failed':
                                  crash_info = test_result[phase].get('crash', {})
                                  traceback_info = test_result[phase].get('traceback', [])

                                  error_type = crash_info.get('message', '').split(':')[0].strip() if ':' in crash_info.get('message', '') else "UnknownError"
                                  # Improve error type if it's a known Python exception pattern
                                  if 'Exception:' in crash_info.get('message', ''):
                                      error_type = crash_info['message'].split('Exception:')[0].strip().split('.')[-1] + 'Exception'
                                  elif 'Error:' in crash_info.get('message', ''):
                                      error_type = crash_info['message'].split('Error:')[0].strip().split('.')[-1] + 'Error'
                                  elif error_type == 'Message' and crash_info.get('message', '').startswith('session not created: '):
                                      error_type = 'SessionNotCreatedException'

                                  file_path = ""
                                  line_num = 0
                                  method_name = ""
                                  class_name = ""
                                  # Use longrepr for comprehensive error, escape for JSON
                                  raw_error_content = crash_info.get('longrepr', '')
                                  raw_error = json.dumps(raw_error_content).strip('"') # Safely encode and remove surrounding quotes

                                  if traceback_info:
                                      # Get the last frame that's not internal selenium/pytest for relevant context
                                      for frame in reversed(traceback_info):
                                          path = frame.get('path', '')
                                          if 'site-packages/selenium' not in path and 'site-packages/pytest' not in path and 'site-packages/_pytest' not in path:
                                              file_path = path
                                              line_num = frame.get('lineno', 0)
                                              # Method and class are often not directly in frame, but can be inferred from nodeid
                                              # For simpler extraction, use method from nodeid and class from nodeid if available
                                              parts = test_result['nodeid'].split('::')
                                              if len(parts) > 2:
                                                  method_name = parts[-1]
                                                  if len(parts) > 3 and parts[-2].startswith('Test'): # Simple heuristic for class
                                                      class_name = parts[-2]
                                              break # Take the first relevant stack frame

                                  # Fallback for raw_error if longrepr is not what we want or missing
                                  if not raw_error_content and os.path.exists(test_output_file):
                                      with open(test_output_file, 'r') as log_f:
                                          log_content = log_f.read()
                                          # Extract E lines from raw log, similar to old bash logic
                                          e_lines = [line[4:].strip() for line in log_content.splitlines() if line.startswith('E   ')]
                                          raw_error = json.dumps(' '.join(e_lines[:3])).strip('"') # Safely encode and limit to first 3 lines

                                  failure_object = {
                                      "type": error_type,
                                      "phase": phase, # setup, call, teardown
                                      "file": file_path,
                                      "line": line_num,
                                      "method": method_name,
                                      "class": class_name,
                                      "rawError": raw_error,
                                      "assertion": {
                                          "available": False, # pytest-json-report doesn't directly give this in simple format
                                          "expression": "",
                                          "expected": "",
                                          "actual": "",
                                          "operator": ""
                                      }
                                  }
                                  # Exit loops once failure is found for this test
                                  break
                          if failure_object:
                              break

              except json.JSONDecodeError as e:
                  print(f"Error parsing JSON report {json_report_file}: {e}", file=sys.stderr)
              except Exception as e:
                  print(f"An unexpected error occurred during failure object extraction: {e}", file=sys.stderr)

          print(json.dumps(failure_object) if failure_object else "null")
          EOF

          # ENHANCED: Send webhook function (same as before, but calls python script for failure object)
          send_webhook() {
              local test_id="$1"
              local status="$2"
              local duration="$3"
              local test_logs_raw="$4" # Raw logs directly from the step output
              local test_output_file="$5"
              local json_report_file="test-results/json-${test_id}.json"

              local failure_object="null"
              if [ "$status" = "Failed" ]; then
                  # Set environment variables for the Python script
                  export CURRENT_TEST_ID="$test_id"
                  export CURRENT_JSON_REPORT_FILE="$json_report_file"
                  export CURRENT_TEST_OUTPUT_FILE="$test_output_file"
                  export CURRENT_TEST_STATUS="$status"
                  failure_object=$(python extract_failure_object.py)
                  unset CURRENT_TEST_ID CURRENT_JSON_REPORT_FILE CURRENT_TEST_OUTPUT_FILE CURRENT_TEST_STATUS
              fi

              # Ensure logs are properly escaped for JSON and limited in length for webhook
              local escaped_logs=$(echo "$test_logs_raw" | head -20 | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-1000)

              if [ -n "$CALLBACK_URL" ]; then
                  cat > "webhook_${test_id}_${status}.json" << EOF
          {
            "requestId": "$REQUEST_ID",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "results": [
              {
                "id": "$test_id",
                "name": "Test $test_id",
                "status": "$status",
                "duration": $duration,
                "logs": "$escaped_logs",
                "failure": $failure_object,
                "execution": {
                  "exitCode": 1,
                  "framework": "pytest",
                  "pytestDuration": $duration,
                  "outputFile": "test-results/output-${test_id}.log"
                }
              }
            ]
          }
          EOF
                  # Send webhook
                  HTTP_CODE=$(curl -w "%{http_code}" -o "response_${test_id}_${status}.txt" \
                    -X POST \
                    -H "Content-Type: application/json" \
                    -H "User-Agent: GitHub-Actions-Quality-Tracker" \
                    -H "X-GitHub-Run-ID: $GITHUB_RUN_ID" \
                    -H "X-Request-ID: $REQUEST_ID" \
                    -H "X-Test-Case-ID: $test_id" \
                    -d @"webhook_${test_id}_${status}.json" \
                    "$CALLBACK_URL" \
                    --max-time 30 \
                    -s)

                  if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
                    echo "âœ… Webhook sent successfully (HTTP $HTTP_CODE)"
                  else
                    echo "âŒ Webhook failed (HTTP $HTTP_CODE)"
                  fi

                  sleep 0.5
              fi

              # ENHANCED: Update consolidated results WITH failure object (use same failure_object variable)
              cat > update_enhanced_consolidated_result.py << EOF
          import json
          import os
          import time

          # Load current consolidated results
          try:
              with open("current_results.json", "r") as f:
                  payload = json.load(f)
          except Exception: # Catch general exception for robustness
              payload = {
                  "requirementId": os.environ.get("REQUIREMENT_ID", ""),
                  "requestId": os.environ.get("REQUEST_ID", ""),
                  "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                  "metadata": {
                      "workflowRunId": os.environ.get("GITHUB_RUN_ID", ""),
                      "executionMode": "enhanced-with-failure-objects",
                      "totalTests": 0,
                      "enhancedArtifacts": True
                  },
                  "results": []
              }

          # Update specific test result in enhanced consolidated format
          test_id = "$test_id"
          status = "$status"
          duration = int("$duration" or "0")
          # The logs passed here are already escaped and limited from the bash function
          logs = "$escaped_logs"

          # Use the failure object already extracted by the Python script
          failure_object_json = "$failure_object" # This will be the JSON string 'null' or the JSON object
          failure_obj = None

          if failure_object_json.strip() and failure_object_json.strip() != "null":
              try:
                  failure_obj = json.loads(failure_object_json)
              except json.JSONDecodeError:
                  failure_obj = None # Keep as None if parsing fails

          # Find and update the test result with enhanced data
          updated = False
          for result in payload["results"]:
              if result["id"] == test_id:
                  result["status"] = status
                  result["duration"] = duration
                  result["logs"] = logs
                  result["name"] = f"Test {test_id}"
                  result["failure"] = failure_obj  # NEW: Include failure object
                  result["execution"]["exitCode"] = 1 if status == "Failed" else 0
                  result["execution"]["pytestDuration"] = duration
                  result["execution"]["outputFile"] = f"test-results/output-{test_id}.log" # Ensure output file is set
                  updated = True
                  break

          # If not found, add new enhanced result
          if not updated:
              payload["results"].append({
                  "id": test_id,
                  "name": f"Test {test_id}",
                  "status": status,
                  "duration": duration,
                  "logs": logs,
                  "failure": failure_obj,  # NEW: Include failure object
                  "execution": {
                      "exitCode": 1 if status == "Failed" else 0,
                      "framework": "pytest",
                      "pytestDuration": duration,
                      "outputFile": f"test-results/output-{test_id}.log"
                  }
              })

          # Update timestamp and metadata
          payload["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
          payload["requirementId"] = os.environ.get("REQUIREMENT_ID", "")
          payload["requestId"] = os.environ.get("REQUEST_ID", "")

          # Enhanced metadata
          if "metadata" not in payload:
              payload["metadata"] = {}

          payload["metadata"]["workflowRunId"] = os.environ.get("GITHUB_RUN_ID", "")
          payload["metadata"]["executionMode"] = "enhanced-with-failure-objects"
          payload["metadata"]["totalTests"] = len(payload["results"])
          payload["metadata"]["enhancedArtifacts"] = True

          # Save enhanced consolidated results
          with open("current_results.json", "w") as f:
              json.dump(payload, f, indent=2)

          print(f"ğŸ“ Updated enhanced consolidated results: {test_id} -> {status}")
          EOF

              python update_enhanced_consolidated_result.py
          }

          # Process each test with enhanced artifact generation
          for test_id in "${TEST_ARRAY[@]}"; do
              if [ -z "$test_id" ]; then continue; fi

              CURRENT_TEST=$((CURRENT_TEST + 1))
              echo ""
              echo "ğŸ§ª [$CURRENT_TEST/$TOTAL_TESTS] Processing test: $test_id"

              # Define output file path
              test_output_file="test-results/output-${test_id}.log"
              json_report_file="test-results/json-${test_id}.json" # Define JSON report file path

              # 1. Send "Not Started" status
              send_webhook "$test_id" "Not Started" "0" "Test queued for execution" "$test_output_file" # Pass output_file
              sleep 0.1 # Small delay to avoid race conditions with webhook processing

              # 2. Send "Running" status
              send_webhook "$test_id" "Running" "0" "Test execution in progress..." "$test_output_file" # Pass output_file
              sleep 0.1 # Small delay

              # 3. Run the test and capture detailed output
              start_time=$(date +%s)

              echo "â–¶ï¸  Executing: python -m pytest -v -k \"$test_id\" --tb=long"

              # Run test with enhanced error capture
              # Capture stdout and stderr to the output file
              if python -m pytest -v -k "$test_id" --tb=long \
                  --junit-xml="test-results/junit-${test_id}.xml" \
                  --json-report --json-report-file="$json_report_file" \
                  > "$test_output_file" 2>&1; then

                  test_status="Passed"
                  echo "âœ… Test PASSED: $test_id"
              else
                  exit_code=$?

                  if [ $exit_code -eq 5 ] || grep -q "collected 0 items" "$test_output_file"; then
                      test_status="Not Found"
                      echo "âš ï¸  Test implementation NOT FOUND: $test_id"
                  else
                      test_status="Failed"
                      echo "âŒ Test FAILED: $test_id (exit code: $exit_code)"
                  fi
              fi

              end_time=$(date +%s)
              duration=$((end_time - start_time))

              # Read raw logs from file for consistency
              test_logs_content=$(cat "$test_output_file")

              # 4. Send final result with enhanced data
              send_webhook "$test_id" "$test_status" "$duration" "$test_logs_content" "$test_output_file" # Pass full logs

              echo "ğŸ“Š Test completed: $test_id -> $test_status (${duration}s)"
              sleep 1
          done

          echo ""
          echo "ğŸ All tests completed with enhanced artifacts!"
        continue-on-error: true

      - name: Generate enhanced final results and summary
        run: |
          echo "ğŸ“Š Generating enhanced final consolidated results..."

          # ENHANCED: Generate enhanced summary with failure analysis
          cat > generate_enhanced_summary.py << EOF
          import json
          import os
          import sys

          summary_content = []
          total_tests = 0
          passed_tests = 0
          failed_tests = 0
          not_found_tests = 0
          total_duration = 0
          failure_details = [] # To store structured failure info

          try:
              if os.path.exists("current_results.json"):
                  with open("current_results.json", "r") as f:
                      results = json.load(f)

                  total_tests = results["metadata"]["totalTests"]
                  summary_content.append(f"## ğŸ§ª Test Execution Summary - Enhanced ({results.get('requirementId', 'N/A')})")
                  summary_content.append(f"**Request ID:** {results.get('requestId', 'N/A')}")
                  summary_content.append(f"**Workflow Run ID:** {results.get('metadata', {}).get('workflowRunId', 'N/A')}")
                  summary_content.append(f"**Timestamp:** {results.get('timestamp', 'N/A')}")
                  summary_content.append("")
                  summary_content.append("### Individual Test Results:")
                  summary_content.append("| Test ID | Status | Duration (s) | Logs |")
                  summary_content.append("|---|---|---|---|")

                  for test in results["results"]:
                      status = test["status"]
                      duration = test["duration"]
                      test_id = test["id"]
                      logs = test["logs"].replace('\n', ' ').strip()
                      # Truncate logs if too long for summary display
                      if len(logs) > 100:
                          logs = logs[:97] + "..."

                      summary_content.append(f"| {test_id} | {status} | {duration} | {logs} |")

                      total_duration += duration
                      if status == "Passed":
                          passed_tests += 1
                      elif status == "Failed":
                          failed_tests += 1
                          # Extract and store detailed failure info
                          failure_obj = test.get("failure")
                          # Check if failure_obj is not None and not the string "null"
                          if failure_obj is not None and failure_obj != "null":
                              failure_details.append({
                                  "id": test_id,
                                  "type": failure_obj.get("type", "UnknownType"),
                                  "phase": failure_obj.get("phase", "N/A"),
                                  "file": failure_obj.get("file", "N/A"),
                                  "line": failure_obj.get("line", "N/A"),
                                  "method": failure_obj.get("method", "N/A"),
                                  "class": failure_obj.get("class", "N/A"),
                                  "rawError": failure_obj.get("rawError", "No raw error available").strip().replace('\\n', ' ').replace('"', "'") # Clean for markdown
                              })
                      elif status == "Not Found":
                          not_found_tests += 1

                  summary_content.append("")
                  summary_content.append("### Overall Statistics:")
                  summary_content.append(f"- **Total Tests:** {total_tests}")
                  summary_content.append(f"- **Passed:** {passed_tests}")
                  summary_content.append(f"- **Failed:** {failed_tests}")
                  summary_content.append(f"- **Not Found:** {not_found_tests}")
                  summary_content.append(f"- **Total Duration:** {total_duration} seconds")

                  if failed_tests > 0:
                      summary_content.append("")
                      summary_content.append("### âŒ Detailed Failure Analysis:")
                      for f_detail in failure_details:
                          summary_content.append(f"#### Test Case: `{f_detail['id']}`")
                          summary_content.append(f"- **Error Type:** `{f_detail['type']}`")
                          summary_content.append(f"- **Phase:** `{f_detail['phase']}`")
                          if f_detail['file'] and f_detail['file'] != "N/A":
                              summary_content.append(f"- **Location:** `{f_detail['file']}:{f_detail['line']}`")
                          if f_detail['method'] and f_detail['method'] != "N/A":
                              summary_content.append(f"- **Method:** `{f_detail['method']}`")
                          if f_detail['class'] and f_detail['class'] != "N/A":
                              summary_content.append(f"- **Class:** `{f_detail['class']}`")
                          summary_content.append(f"- **Raw Error Snippet:**")
                          summary_content.append(f"```\n{f_detail['rawError'][:500]}{'...' if len(f_detail['rawError']) > 500 else ''}\n```") # Limit raw error for summary
                          summary_content.append("") # Add a blank line for readability

                  summary_content.append("")
                  summary_content.append("---")
                  summary_content.append("**Summary of Enhanced Features:**")
                  summary_content.append("- ğŸ“Š **Failure Objects in Artifacts:** Complete debugging info preserved (Error Type, Location, Raw Message)")
                  summary_content.append("- ğŸ“ˆ **Detailed Analysis:** Easy identification of failed tests and root causes.")
                  summary_content.append("- ğŸ“¦ **Enhanced Metadata:** Workflow and request context for better traceability.")
                  summary_content.append("- ğŸ“¡ **Webhooks:** Automated notifications with rich failure data.")
                  summary_content.append("- ğŸ’¾ **Backup Debugging:** Full failure information available even if webhook delivery fails.")

                  # Write to summary.md file
                  with open("summary.md", "w") as f:
                      for line in summary_content:
                          f.write(line + "\n")

              else:
                  print("âŒ No current_results.json found to generate summary.")

          except Exception as e:
              print(f"An error occurred during summary generation: {e}", file=sys.stderr)
              print("âŒ Failed to generate enhanced summary due to an internal error.")

          EOF

          if [ -f "current_results.json" ]; then
            # Create enhanced results with run ID
            cp current_results.json "results-$GITHUB_RUN_ID.json"
            echo "âœ… Enhanced results file created: results-$GITHUB_RUN_ID.json"

            # Display enhanced consolidated results
            echo "ğŸ“„ Enhanced consolidated results with failure objects:"
            cat current_results.json | jq '.' || cat current_results.json

            # Run the Python script and pipe its output to GITHUB_STEP_SUMMARY
            python generate_enhanced_summary.py
            # Now, explicitly cat the generated summary.md to $GITHUB_STEP_SUMMARY
            if [ -f "summary.md" ]; then
                cat summary.md >> $GITHUB_STEP_SUMMARY
            else
                echo "âŒ summary.md file was not generated." >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ No enhanced consolidated results found" >> $GITHUB_STEP_SUMMARY
            echo "âŒ No enhanced consolidated results found"
          fi

          # Clean up temporary files
          echo "ğŸ§¹ Cleaning up temporary files..."
          rm -f webhook_*_*.json response_*_*.txt update_enhanced_consolidated_result.py generate_enhanced_summary.py initialize_results.py extract_failure_object.py
          echo "âœ… Cleanup completed"

      - name: Upload Test Results Artifact
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: test-results/
          retention-days: 7

      - name: Upload Enhanced Consolidated Results Artifact
        uses: actions/upload-artifact@v4
        with:
          name: enhanced-consolidated-results-${{ github.run_id }}
          path: results-${{ github.run_id }}.json
          retention-days: 7

      - name: Upload Enhanced Summary Artifact
        uses: actions/upload-artifact@v4
        with:
          name: enhanced-summary-${{ github.run_id }}
          path: summary.md
          retention-days: 7